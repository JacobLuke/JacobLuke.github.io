<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jacob's Blograhams</title><link href="http://jabrahams.ca/" rel="alternate"></link><link href="http://jabrahams.ca/feeds%5Call.atom.xml" rel="self"></link><id>http://jabrahams.ca/</id><updated>2019-01-01T00:00:00Z</updated><entry><title>Coffee And Me: A Complicated Romance</title><link href="http://jabrahams.ca/caffeine.html" rel="alternate"></link><updated>2019-01-01T00:00:00Z</updated><author><name>Jacob Abrahams</name></author><id>tag:jabrahams.ca,2019-01-01:caffeine.html</id><summary type="html">&lt;p&gt;This is a tale in two parts about how I quit, then subsequently un-quit, caffeine, and the glaringly-obvious-in-hindsight-yet-still-meaningful things I learned about my brain along the way. Come with me and find out some facts anyone with a passing understanding of even the simplest parts of neurobiology could tell you, all of which I found out after &lt;em&gt;two&lt;/em&gt; separate month-long adjustment periods and a hell of a lot of &lt;em&gt;arguably avoidable&lt;/em&gt; strife. It'll be fun!&lt;/p&gt;
&lt;h2&gt;Part 1: Oct 2015 - Mar 2017&lt;/h2&gt;
&lt;p&gt;A few months after I started full-time at &lt;code&gt;BIG_COMPANY&lt;/code&gt;, I participated in a blood drive they were hosting on campus. This was nothing new for me - giving blood is one of my favourite forms of charity [1], and I'd done it roughly five times in the previous few years. What &lt;em&gt;was&lt;/em&gt; new was something the technicians warned me about: my blood pressure was pretty dangerously high for an otherwise-healthy young adult male, around 150 systolic as I recall. I'd had problems with blood pressure (among other things) during my less-than-healthy (read: fat) pubescence, but this wasn't pudgy-short Jacob, this was &lt;em&gt;just-biked-across-fucking-Europe&lt;/em&gt; Jacob! Something was clearly out of balance.&lt;/p&gt;
&lt;p&gt;The staff took my blood, but not before warning me to get a professional opinion from a doctor. Conveniently, there was one near work, but instead of that I treated the whole thing as I would a bug and Googled the quickest way to fix the problem. One module-at-the-top-of-the-search-results later, I'd narrowed down the potential causes to the following candidates:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Stress&lt;/li&gt;
&lt;li&gt;Alcohol&lt;/li&gt;
&lt;li&gt;Greasy Food&lt;/li&gt;
&lt;li&gt;Caffeine&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I eliminated the first three as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I was currently the least-stressed I'd be in my entire professional life (at least at the time of this writing), because I was treating my job as I had treated my internships and the &lt;em&gt;horrendous downsides of doing that&lt;/em&gt; wouldn't become clear to me for a few months yet.&lt;/li&gt;
&lt;li&gt;I'd already cut down harshly on my drinking a few months prior, and doing more would just hurt me socially at that point.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Probably&lt;/em&gt; valid, but &lt;code&gt;BIG_COMPANY&lt;/code&gt; food was so good, I'd rather just have the high blood pressure, thanks.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Basic deduction therefore left caffeine as the only possible culprit [2]. I may have already been considering this as the culprit before my extremely detailed research (read: a Google search which was lazy even by Google search standards) because at the time of the measurement I was drinking, on average, 4 coffees per day, half of which would be sugary varieties like Mochas. I knew this wasn't &lt;em&gt;good for me&lt;/em&gt; per se, but until now I hadn't realized that it was &lt;em&gt;actively bad for me&lt;/em&gt;. So I did the only reasonable thing.&lt;/p&gt;
&lt;p&gt;I cut out caffeine. All of it, all at once.&lt;/p&gt;
&lt;p&gt;I bet you're going, "Yeah, but you were drinking coffee again within the month, I bet."  Keep in mind, I'd had a love-hate-abuse-quit-relapse relationship with coffee for the better part of a decade at this point, and this wasn't my first time "quitting forever",  so even &lt;em&gt;I&lt;/em&gt; had that reaction. But I was wrong, and so are you. I stuck to this dumbass idea for &lt;em&gt;over two years&lt;/em&gt;, and I didn't even really feel tempted to break away from it until a few days before I did.&lt;/p&gt;
&lt;p&gt;If you're wondering, here's my "Super Effective, Guaranteed to Work (Or Maybe Kill You In the Process) Cold Turkey Method For Quitting Caffeine" (patent pending), in case you want to reproduce my results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Don't drink coffee. This may come across as obvious, but I mean &lt;em&gt;all&lt;/em&gt; coffee. No decaf, no chocolate-covered coffee beans, nothing coffee flavoured. It'll just make you realize how much worse whatever you're consuming is than the real thing, and then it's all over for you. Hell, I even stayed away from tiramisu for a while, because I was so committed to this aspect.&lt;/li&gt;
&lt;li&gt;Don't drink tea. This is a good rule for two reasons: first, a lot of tea is caffeinated to some degree (and hence kind of against the point), and second, tea is just &lt;em&gt;worse coffee&lt;/em&gt; if we're being perfectly honest, which means we run into the same reasoning as we did with decaf.&lt;/li&gt;
&lt;li&gt;Don't drink &lt;em&gt;any&lt;/em&gt; hot beverages for the first month. This goes along the same rationale as the previous points, but is a hell of a lot harder, especially when you occasionally live in a cold climate, where the relief of hot chocolate is &lt;em&gt;so&lt;/em&gt; tempting (luckily, I &lt;em&gt;mostly&lt;/em&gt; avoided this by spending the first few months of the experiment in the land of unreasonable sun).&lt;/li&gt;
&lt;li&gt;Switch to obsessively drinking water. Smokers and former alcoholics usually pick up a new habit to fill the void, so make yours a healthy one. It won't wake you up quite the same way, but you can pretend it does!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Last a month, and you're probably permanently good! Survive a trip over to a remote office in Great Britain where you don't sleep on the plane (as I &lt;em&gt;intelligently decided&lt;/em&gt; to do) and are therefore a zombie for half the time you're away, and you'll probably forget coffee exists!&lt;/p&gt;
&lt;p&gt;Suffice to say, my timing could have been better.&lt;/p&gt;
&lt;p&gt;Nonetheless, I persevered, and stuck to my guns rejecting the one substance that had been a source of joy for me since my early teens. While it didn't &lt;em&gt;actually&lt;/em&gt; end up lowering my blood pressure [3], I did notice an interesting range of effects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As long as I kept my sleep schedule consistent, even if it was incredibly poorly-reasoned and unnatural [4], sticking to it was basically trivial because suddenly my energy levels naturally followed a static, unchanging curve.&lt;/li&gt;
&lt;li&gt;My general anxiety levels, which had been middling for a few years at this point, dropped sharply and pretty much constantly stayed low. [5]&lt;/li&gt;
&lt;li&gt;I was drinking a lot more water to compensate, and water's generally got a ton of good side effects (better digestion, clearer skin, lower heartrate, not dying of dehydration, etc.), up until the point where you're actually drowning and the only side effect is "water in lungs"&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, all of those things paled in comparison to the effect this whole thing had on my mood. If you've read any of my not-about-forks blog posts, you'll know I've suffered from chronic depression for pretty much all of my adult life. In the couple of years prior to this change, it'd taken the following general form: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A few months of barely affecting me&lt;/li&gt;
&lt;li&gt;Then a month or so where it &lt;strong&gt;really&lt;/strong&gt; affected me&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This was bad for two reasons. First, it was unpredictable as fuck, so I had to account for both the possibility I'd be normal and the possibility I'd be &lt;em&gt;barely functional&lt;/em&gt; when trying to plan pretty much anything significant (which was suboptimal to say the least). Second, the down-spike segments were progressively getting &lt;em&gt;worse&lt;/em&gt; with each one, and I was handling them with unevenly successful methods even at the best of times.&lt;/p&gt;
&lt;p&gt;Suddenly, it wasn't doing that. Instead, it looked like this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A motivational lull lasting a few weeks&lt;/li&gt;
&lt;li&gt;...ramping up over the course of a few months...&lt;/li&gt;
&lt;li&gt;...to a normal-ish level, lasting a few weeks again...&lt;/li&gt;
&lt;li&gt;...which gradually descends over a few months back down to the low&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'm not saying, "Quitting Caffeine Cured My Depression." (It really didn't.) I'm saying, "Taking A Psychoactive Substance In High Doses Isn't Always Good For Mental Stability." Quitting caffeine turned off the problems with my depression's cycle &lt;em&gt;hard&lt;/em&gt;, making it less dangerous and more predictable all at once. Now, I knew with reasonable certainty when I'd be functioning like my normal self, and when I wasn't able to do that I knew roughly how long I'd have to pretend. Success!&lt;/p&gt;
&lt;p&gt;This last reason is why I stuck with the whole no-caffeine thing as long as I did. Fuck getting up at 5AM reliably, fuck blood pressure, and fuck healthy consumption habits. Predictable mental states are my &lt;em&gt;jam&lt;/em&gt;, son.&lt;/p&gt;
&lt;p&gt;I didn't really see any downsides to all of this. It'd take a &lt;em&gt;lot&lt;/em&gt; of shit to go down before I would.&lt;/p&gt;
&lt;h2&gt;Part 2: Apr 2017 - Dec 2017 and onward&lt;/h2&gt;
&lt;p&gt;I can hear you asking, "Why did you cut up the time periods like this? You said you stuck to this plan for over two years, but the first one is less than two years long. Plus the end of this part doesn't really make any sense since it's simultaneously a year ago and nonexistent, so what gives?" First off, that's a really detailed question, and I'm glad you're paying attention. Second off, I don't tell you how to do whatever you're doing right now [6], so don't tell me how to define my time ranges. Third, these time ranges correspond not &lt;em&gt;just&lt;/em&gt; to my experiences to "everyone's favourite substance violating the 'I before E' rule," but also experiences in my professional life: in March of 2017 I left &lt;code&gt;BIG_COMPANY&lt;/code&gt; and moved back to Toronto, and I joined &lt;code&gt;SMALL_STARTUP&lt;/code&gt; a month later.&lt;/p&gt;
&lt;p&gt;Then, in September, I got fired. Whoops!&lt;/p&gt;
&lt;p&gt;I don't &lt;em&gt;really&lt;/em&gt; want to detract from the subject by getting into this in depth (trust me, I've had a blog post brewing on that topic for &lt;em&gt;over a year&lt;/em&gt; at this point), but if I can sum up some of the personal lessons I learned from all this, it would go like this: at &lt;code&gt;SMALL_COMPANY&lt;/code&gt;, I'd spent most of the time unmotivated, easily distracted and slow to accomplish most substantial things, slower to communicate, and even &lt;em&gt;slower&lt;/em&gt; to correct course on any of this. This didn't necessarily &lt;em&gt;cause&lt;/em&gt; a toxic relationship with my superiors, but it certainly didn't help when one started to develop. [7]&lt;/p&gt;
&lt;p&gt;Hmm, low motivation, low emotional energy, low mood? Why does that sound familiar?&lt;/p&gt;
&lt;p&gt;Remember how I described the top level of the new arc my depression followed as "normal-ish"? Turns out, that "ish" is pretty important. Despite having a more-or-less even emotional temperment now, it had a &lt;em&gt;lower&lt;/em&gt; centre than ever before, and was less resistant to things that dropped that further. (While maybe also being more resistant to factors that raised it? Hard to say.) I can think of a &lt;em&gt;bunch&lt;/em&gt; of reasons why this suddenly became a factor when it hadn't been in the past:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Big companies have a lot more structure than small ones, and leaning on structure means that you always have a clear next step, so low motivation doesn't generally hurt as bad. [8][9]&lt;/li&gt;
&lt;li&gt;Big companies have constant weekly check-ins and quarterly goals to hit. Smaller ones just expect you to get shit done without any of that guidance.&lt;/li&gt;
&lt;li&gt;Being in a small satellite office of a small company means there aren't as many people around to siphon social energy off of [10], which I find recharges me despite being vastly more introverted than the general population&lt;/li&gt;
&lt;li&gt;Things were kind of going to hell in my personal life, I was less fulfilled in my current role at work, and I'd stopped exercising and eating healthy the moment it became inconvenient, so a bunch of extraneous factors were pushing down on my mood &lt;em&gt;really hard&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Colder, darker climates are harder to function in. I know there's &lt;a href="https://www.mayoclinic.org/diseases-conditions/seasonal-affective-disorder/symptoms-causes/syc-20364651"&gt;a term&lt;/a&gt; for that, but I'm not one to self-diagnose. [11]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Still, after all this, I didn't even consider going back to caffeine at first. Clearly it had been a factor, but despite it all, I was happier without it...&lt;/p&gt;
&lt;p&gt;...right?&lt;/p&gt;
&lt;p&gt;Fast-forward two months of moping, working out, and drinking heavily at a Cuban all-inclusive [12]. I've gotten back up, gussied myself up a bit, and interviewed around in the slightly sparse (and suprisingly interconnected) Toronto tech scene. In the process, I've narrowed my next move down to three offers I have on the table, and after several days thinking it over, one of them is pushing me for an answer, like, &lt;em&gt;today&lt;/em&gt;. In trying to push my brain to function at high capacity, I realize that, while swimming passively along with my brain's natural currents is fine for some things, I need a &lt;em&gt;surfable wave&lt;/em&gt; right now to get shit done. After struggling with the new meta-decision facing me for &lt;em&gt;hours&lt;/em&gt;, I decide what needs to be done.&lt;/p&gt;
&lt;p&gt;I need a coffee.&lt;/p&gt;
&lt;p&gt;After wandering up and down Roncesvalles Ave in Toronto for a good hour, I go to a cute little Polish bakery near the South end of the road, and get a Medium Dark Roast [13]. After a few hours and several whiteboards of Pros and Cons later [14], I'm able to make a decision [15]. &lt;/p&gt;
&lt;p&gt;Thank you, blessed caffeine.&lt;/p&gt;
&lt;p&gt;After this, I grab a cup every few days, finding comfort in the lightened version of my old routines. Later, when I start at the job I selected, I ramp up to my old two-to-three-a-day habit. I then have to &lt;em&gt;tweak&lt;/em&gt; this slightly, because I got old and now can't sleep properly if I drink coffee after dark. (Human biology: it sucks sometimes!) Apart from that, I've stuck to the new schedule &lt;em&gt;pretty much&lt;/em&gt; unchanged since, with occasional rest days to let my brain chemicals reset. In the process, I've noticed a few things I thought were negatives, but are actually subtle tradeoffs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Remember how I said I didn't get anxious anymore, but my general mood was lower? Turns out, you sort of trade one against the other: higher energy and higher volatility naturally go hand in hand&lt;/li&gt;
&lt;li&gt;You also make a tradeoff on &lt;em&gt;how&lt;/em&gt; you want your energy levels: even but predictable, or not-necessarily-predictable but available-on-demand. The first one is better for &lt;em&gt;planning&lt;/em&gt; around, the second is way better at &lt;em&gt;reacting&lt;/em&gt; to the unpredictable. &lt;code&gt;BIG_COMPANY&lt;/code&gt; was fine with the first, but startups really need the second.&lt;/li&gt;
&lt;li&gt;At the same time, the mood stability didn't really go away. Maybe I was just jumping to conclusions and shouldn't have been making assumptions during a time when I'd recently been &lt;em&gt;super depressed&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Fun fact: you can drink coffee &lt;strong&gt;and&lt;/strong&gt; water! No need to quit your vice for that, you can glean the benefits of both at the same time!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So where does that leave us? Well, it's 11:30pm on a Tuesday and I had to stay up late the night before (fucking New Years, worst holiday). Without coffee, I'd be unable to do anything more than browse my phone in bed (or, y'know, sleep). Instead, I'm writing this conclusion to an article that I've been meaning to write for months but never got around to, and I have a late-afternoon Mocha to thank for that. What we choose to put in our bodies is always some kind of tradeoff: food that's healthy is often devoid of joy, alcohol makes people bearable but the next morning less so, and most of the other things we enjoy just serve to hurt us in some way or another. Caffeine is no different here. [16]&lt;/p&gt;
&lt;p&gt;It really just comes down to a choice: do you want to ride the waves or try to guide them?&lt;/p&gt;
&lt;p&gt;J&lt;/p&gt;
&lt;h2&gt;Notes:&lt;/h2&gt;
&lt;p&gt;[1]: It's free, relatively quick, and &lt;em&gt;metal as fuck&lt;/em&gt;. Plus they give you a cookie and juice!
[2]: Basic deduction would end up being wrong. When I did &lt;em&gt;eventually&lt;/em&gt; go to a doctor about all this, the cause they deduced was that I get mild anxiety from medical procedures (like, say, having my blood pressure taken), and that was messing up my vitals. Turns out, the internet is no substitute for good healthcare. Go figure.
[3]: See point 2. I'm only a few rungs above people who rely on WebMD. 
[4]: It was. Partially to keep more-in-sync with the East coast (where my family and girlfriend lived), partially to avoid horrendous Bay Area traffic, and partially just because, I'd committed to being out the door by 6AM every workday, and I managed to keep that up the entire time I lived in SF. Nowadays, the mere idea of that baffles me, and I sometimes can't get out of bed at 8AM. Brains are weird!
[5]: In retrospect, a) this should have been an obvious consequence, and b) this really should've tipped me off to [2] a lot sooner than it did. Whoops.
[6]: Despite the fact that I can hear you ask this, I can't actually see what you're doing. My apologies.
[7]: Another big factor, which I &lt;em&gt;will&lt;/em&gt; mention despite being besides the point: I joined &lt;code&gt;SMALL_COMPANY&lt;/code&gt; not primarily because of the people or the tech, but rather because they were located in SF, along with my social circle, but were opening a satellite in Toronto, where I was moving. Whenever asked for reasons why I joined in interviews, I absolutely &lt;em&gt;struggled&lt;/em&gt; not to mention this one first. Pro-tip: logistics is not a good main reason to join a company.
[8]: I'm generalizing a bunch here, obviously, and learning to self-structure is a huge thing I picked up from this experience, on top of everything else.
[9]: There actually &lt;em&gt;was&lt;/em&gt; a time where low motivation almost fucked me over at &lt;code&gt;BIG_COMPANY&lt;/code&gt;, but I didn't connect that to this until...just about now?
[10]: For most of my time there, there was &lt;em&gt;one&lt;/em&gt; other person. 
[11]: My depression isn't self-diagnosed, for the record.
[12]: Some of which were healthier than others, obviously.
[13]: Sadly, I think this place has gone under since I moved. One time, they had what I would go on to describe as some of the most perfect croissants I've ever experienced (and I know my croissants). The next, they had the crappy ones that are basically just dishonest bread. I'll chock up their failure to the croissants.
[14]: Sometimes I'm more of a stereotypical engineer than I mean to be.
[15]: I'm still at this company at the time of this writing (just over a year later), so it could have been a worse decision I guess?
[16]: Also it cannot be stressed enough: &lt;strong&gt;THE INTERNET IS NO SUBSTITUTE FOR A MEDICAL OPINION&lt;/strong&gt;&lt;/p&gt;</summary><category term="coffee"></category><category term="mental health"></category><category term="nerdish"></category><category term="personal"></category></entry><entry><title>Selenophobia</title><link href="http://jabrahams.ca/majora.html" rel="alternate"></link><updated>2015-02-16T00:00:00Z</updated><author><name>Jacob Abrahams</name></author><id>tag:jabrahams.ca,2015-02-16:majora.html</id><summary type="html">&lt;h2&gt;Impulse purchases and the Love of Gaming&lt;/h2&gt;
&lt;p&gt;I'd been on the fence about buying Majora's Mask 3D. No, really. My 3DS was mostly out of SD-card space (stupid memory-hogging Omega Ruby), and I didn't think I could justify an in-person purchase of a physical cartridge as easily as I could a digital download. That being said, when I stepped off the Greyhound bus, I knew that between the terminal and the nearest subway station lay a Game Shack, and my bank account would not likely be passing by unmolested.&lt;/p&gt;
&lt;p&gt;So I caved. Oh well. Happy Valentine's Day, me.&lt;/p&gt;
&lt;p&gt;Later, at home, I popped the cartridge into my beleaguered 3DS-XL, figuring that, with my 6 year-old half-brother and his friend B distracted by some TV show or another, I would have some privacy to get reacquainted with the (somewhat literal) dark horse of the Zelda franchise. No such luck. Having been the main proponent of getting Noah into video games (going so far as to purchase his 2DS and half his library), I should have realized that he would be glued to my screens before I could even create a save file.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ohshitohshitohshit&lt;/em&gt;, I'm now thinking, &lt;em&gt;These kids are going to get all freaked out and terrified and then they're going to get nightmares and get scared of the moon and then they'll never play video games again and then-&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;And then nothing. Noah and B were fine with it. &lt;/p&gt;
&lt;p&gt;Eerily, in fact. I'd stop and talk to them through the run through the first 3-day cycle and try to gauge their reactions to things that put even me on edge.&lt;/p&gt;
&lt;p&gt;"Look, he's surrounded by tree-people. They're piling around him, burying him. Doesn't that freak you out, even just a little?"&lt;/p&gt;
&lt;p&gt;"No, it's just silly."&lt;/p&gt;
&lt;p&gt;"Now I'm trapped in the body of one of the tree things. (It's not a spoiler if it's 5 minutes into the game.) Isn't that scary?"&lt;/p&gt;
&lt;p&gt;"Not really."&lt;/p&gt;
&lt;p&gt;"Look at that guy, his weird, stretched smile, his anxiously clasped hands. Isn't he a little creepy?"&lt;/p&gt;
&lt;p&gt;"He's just funny."&lt;/p&gt;
&lt;p&gt;The only time I elicited some reaction was when I (semi-deliberately) lingered the camera on the creepy-faced moon for several (in-game) minutes. Even then, whatever fear/unease that caused was immediately followed by, "I wonder what'd happen if we &lt;em&gt;let&lt;/em&gt; it hit us." Around then, it hit me: I'd expected the scary stuff to hit them harder because they were younger, but maybe it was the opposite; maybe they were &lt;em&gt;too young&lt;/em&gt; to even really see why these things were scary.&lt;/p&gt;
&lt;h2&gt;This is for kids?&lt;/h2&gt;
&lt;p&gt;My first real attempt at finishing the original version of Majora was around 2012. It had come in the Zelda Master Collection which was packaged with the Gamecube I'd gotten when I was 11, and I'd elected to bring the disc up to my Waterloo apartment alongside my Wii almost as an afterthought. I'd had some time to kill in the gap between the end of classes and starting co-op, so I popped in the collection, deciding not to renew the efforts on Ocarina of Time that I'd made on an emulator (which, if memory serves, were abandoned due to not having the spare time to properly defeat the Water Temple) and, instead, see what the darker younger brother had to offer.&lt;/p&gt;
&lt;p&gt;That stopped soon after.&lt;/p&gt;
&lt;p&gt;The "official" internal reason was that I needed to concentrate on work, and the game was interfering with restful downtime. The real reason, though, had less to do with other commitments and more to do with the deep, existential fears the game was dredging up in me every time the moon started falling from the sky.&lt;/p&gt;
&lt;p&gt;Confused? Let me explain. After the success of Ocarina of Time, Nintendo wanted to continue the new trend in more complex, three-dimensional Zelda games while minimizing the amount of extra work they'd have to do to produce a new title. With these goals in mind, they decided to make a game with an entirely new storyline and as many reused assets from OoT as possible. In Majora's Mask, you play a young Link who is thrust into a new world, Termina, which is populated by doppelgangers of characters from the first game and a very angry moon. Transformed into a herbaceous creature known as a Deku, you have 3 in-game days to find the child who transformed you and return his evil-powered mask to its rightful owner, lest he crash the aforementioned creepy satellite into your new home, killing you and all of the people you know and love in the process.&lt;/p&gt;
&lt;p&gt;Spoiler: you fail! But don't worry: you get your magical-time-control flute back from the villain, and, with moments to spare, you send yourself back in time to prevent any of the plot from happening...&lt;/p&gt;
&lt;p&gt;Just kidding! You send yourself to the beginning of the first day, so you can start the whole process all over again. And again. And again.&lt;/p&gt;
&lt;p&gt;The whole thing works out to something &lt;em&gt;immensely&lt;/em&gt; more gruesome than its predecessor. The constant, lunar spectre of death always gazing upon you, inching closer with every second you waste; the overall bizarre, surreal feeling of being in a world that's almost like a dream of the one you know; the frantic race against the clock, eyes always on the countdown so you know when it's time to flee time itself and return to the start of things, abandoning important possessions in the hopes that you might live to reacquire them. Moreover, the original version had no in-game saves; the only way to retain your progress was, ironically, to rewind time and &lt;em&gt;abandon&lt;/em&gt; at least a small portion of it. To think that you couldn't even &lt;em&gt;stop&lt;/em&gt; playing without abandoning some alternate timeline to be crushed under the sky itself added even &lt;em&gt;more&lt;/em&gt; anxiety to even turning the console on.&lt;/p&gt;
&lt;p&gt;Or, at least, it did for me.&lt;/p&gt;
&lt;h2&gt;Adult Fears&lt;/h2&gt;
&lt;p&gt;I think the terrors of Majora's Mask are legion, but most of them can be explained in some way or another to the most prevalent feature: the glaring, falling moon. First and foremost, the moon gives us a sense of the not-quite-right; it's something we recognize (as long as you've ever seen nighttime), and yet it definitely &lt;em&gt;isn't&lt;/em&gt;. (Last I checked, the moon had a stable-ish orbit and no anthropomorphic face, be it docile or enraged.) This extends to the game, of course: everyone you meet is &lt;em&gt;almost&lt;/em&gt; someone you know (from OoT), but there's just &lt;em&gt;something&lt;/em&gt; off about each one of them. A slightly different personality, maybe, or a slightly sadder history, or even just an unexpected voice. Hell, this is the big factor behind the Mask Salesman's vibes: apart from the fact that he always seems to know just a &lt;em&gt;little&lt;/em&gt; bit too much (being one of 4 characters immune to the restarting timeline), he just feels...&lt;em&gt;wrong&lt;/em&gt; compared to the other Terminans, a touch too much like the masks he wears and not enough like the other &lt;em&gt;people&lt;/em&gt; you encounter. It's an Uncanny Valley effect tuned to subtle, subversive note. The player gets the feeling that the world they inhabit is inherently alien despite (or, perhaps, due to) its familiarity, and that lasts throughout the quests and errands. &lt;/p&gt;
&lt;p&gt;Then there's the obvious source of panic: the fact that this familiar rock in the sky is &lt;em&gt;no longer staying put&lt;/em&gt;, and will in fact destroy everything you (think you) know and love within 72 hours. You get this (perfectly executed) sense that &lt;em&gt;time itself&lt;/em&gt; is no longer an ally, or even a neutral observer, but is now a direct and deadly opponent consciously acting against your efforts. Think about those nightmares you get of an assignment deadline approaching far too quickly, the fear of the unfinished essay some people get decades after their last class. The feeling MM gives you is &lt;em&gt;this&lt;/em&gt; personified, and I can personally attest to it staying with you hours after the console is turned off. It's an atmosphere of panic and hurry that hasn't really ever been revisited in the Zelda franchise, and it really makes its mark here as something &lt;em&gt;different&lt;/em&gt;, something not to be ignored but felt and embraced as a true fear that only this game can provide.&lt;/p&gt;
&lt;p&gt;The last kind of fear I'm going to discuss is a bit more of a stretch, but bear with me. In OoT, the writers imply that time travel works off of something like the Many Worlds model: you jump between two parallel universes, one where you were run around as a child and another where you are forced into a seven-year slumber by the biggest asshole in all of gaming's sentient weaponry and act against Gannondorf as an adult. Critically, Nintendo has also officially stated the existence of a third timeline where, at some point in your journey, you die to some minor enemy and don't get back up again. This third timeline is a great way of guilting players who give up on a &lt;em&gt;certain&lt;/em&gt; temple and just abandon their savefile to the aether, and it's also a great way out of the bind Nintendo accidentally worked themselves into while trying to tie all the games together.&lt;/p&gt;
&lt;p&gt;Now, think about how time travel works in Majora's Mask. Instead of flipping between fixed points in two separate timelines, you are constantly moving back to the beginning of the same timeline, and thereby branching it off. However, there's no reason that the timeline you left would &lt;a href="http://tvtropes.org/pmwiki/pmwiki.php/Main/NoOntologicalInertia"&gt;just cease to exist&lt;/a&gt;. Sure, you make your way back to the Dawn of the First Day just before the moon fellates Clock Town (ick), but the other villagers? They don't exactly have magical wormhole-generating flutes. &lt;/p&gt;
&lt;p&gt;In other words, every time you play that Song of Time, you make another "third timeline". Another universe of people who didn't have the Hero of Time to save them in their time of need, or who didn't quite have the right things set up at the right time. Another entire town of people left to die because you needed one more shot to get it right. See the real terror behind Termina's moon isn't the fear of the passage of time, or the fear that something is just &lt;em&gt;wrong&lt;/em&gt;, but the fear that, despite all the power at your disposal, despite every resource you're given and every skill you acquire, despite the &lt;em&gt;universe itself&lt;/em&gt; bending to help you succeed, you'll still fail to protect the people you care about, the people who need you, the people crushed beneath an unavoidable and unforgiving rock.&lt;/p&gt;
&lt;p&gt;There's another side to &lt;em&gt;this&lt;/em&gt; fear that's something a little more personal. See, a cycle in Majora's Mask (if played to full) lasts 3-9 hours (depending on application of certain songs). When you roll back the clock, everyone goes back in age by three days, whatever that means to them. Everyone...except you. With each 6-note song on the Ocarina, you lose those three "days". Each failure to win on a cycle is a death of a little part of yourself, something &lt;em&gt;real&lt;/em&gt; to go along with the deaths of all those fictional people in the doomed and forgotten universes. Link can keep playing that song as many times as he pleases, but when your bones are too brittle and weak to move the C-stick, when your eyes are too blurred to see the stave you need to fill, that's when you let down the character who needed you most.&lt;/p&gt;
&lt;h2&gt;A Terrible Fate&lt;/h2&gt;
&lt;p&gt;So why don't little kids understand these fears, anyway? What makes these concepts so inaccessible to a 6 year-old. Dammit, &lt;em&gt;why is the Happy Mask Salesman funny to them&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;Well, a lot of the fears I mentioned are built into us as we figure out more of the world around us. The Uncanny Valley effect relies on having a solidified model of the world such that it can be subtly violated, and maybe children that young don't have that yet. (It would certainly explain why the horrendous CGI in films like The Polar Express exists.) Fear of short-term progression of time is really a fear of unavoidable responsibilities, and kids definitely don't really have &lt;em&gt;those&lt;/em&gt;. Fear of letting down the people who are counting on you is something only the most burdened elementary school children have. And fear of aging, fear of time itself? That's a concept that no child should understand, since, as far as anyone is concerned, time is always on their side. &lt;/p&gt;
&lt;p&gt;So, maybe, Majora's Mask actually gets &lt;em&gt;scarier&lt;/em&gt; the older you get, despite the general trend in the opposite direction. Maybe you really need to see what time is capable of before learning to fear it. Maybe you need to really know the moon's face before truly understanding why you don't want to see it closer up.&lt;/p&gt;
&lt;p&gt;Or maybe I should have spent less time reading about &lt;a href="http://creepypasta.wikia.com/wiki/BEN_Drowned"&gt;Ben Drowned&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In any case, I need to get back to my game. Or sleep, since it's 3AM. &lt;/p&gt;
&lt;p&gt;Either way, I know the moon's staying right where it is if it knows what's good for it.&lt;/p&gt;
&lt;p&gt;-J&lt;/p&gt;
&lt;p&gt;PS. If your thirst for Majora's Mask-related theorizing isn't sated, one of my favourite videos on the internet is about &lt;a href="http://www.youtube.com/watch?v=7S1SVkysIRw"&gt;Link secretly being dead for the entire game&lt;/a&gt;. Check it out if &lt;em&gt;you're&lt;/em&gt; not planning on sleeping.&lt;/p&gt;</summary><category term="video games"></category><category term="theory of mind"></category><category term="nerdish"></category></entry><entry><title>Typesafe, Threadsafe Generics in C: A Tale of hubris</title><link href="http://jabrahams.ca/cmap.html" rel="alternate"></link><updated>2014-08-04T00:00:00Z</updated><author><name>Jacob Abrahams</name></author><id>tag:jabrahams.ca,2014-08-04:cmap.html</id><summary type="html">&lt;p&gt;When I learned programming, I learned C about 4-6 months before C++ (and Java about 2 years before both as I've mentioned &lt;a href="java.html"&gt;previously&lt;/a&gt;). I never really noticed, but this had some hand in shaping how I look at problems: I'm more liable to stick to imperative, light-OO programs, more comfortable with dealing with raw memory, and more familiar with rolling my own...well...everything. Essentially, I program to whatever higher-level functionality is present, but don't miss a beat when something's missing.&lt;/p&gt;
&lt;p&gt;It can also make me bite off a lot more than I can chew at times.&lt;/p&gt;
&lt;p&gt;Consider an example. In a Computer Networks course I took this summer, we were assigned to implement a client-server protocol ontop of some (not very performant) infrastructure. The details aren't important, save for one: all of the backing code, including the test code we had to link against, was in C. A smart, sane programmer might go, "Oh, I definitely need some more complex data structures, let's just &lt;code&gt;extern "C"&lt;/code&gt; the code and write some wrappers and use C++ like a normal person."&lt;/p&gt;
&lt;p&gt;We didn't do that.&lt;/p&gt;
&lt;p&gt;In a move lauded by many as "crazy" and "idiotic", we decided to write the code in C. This could have turned out &lt;em&gt;far&lt;/em&gt; worse (most of our bugs actually resulted in starting the assingment late, rather than "We could do this better in C++"), but did leave us with one problem: for a surprising number of parts of the system, we needed a map. Now, most people would stop here and switch to C++, foreseeing the sheer number of type-incompatibilities, segfaults, and other horrendous yet subtle bugs that would be completely avoided using a &lt;code&gt;std::map&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I am not most people.&lt;/p&gt;
&lt;h1&gt;OO in C&lt;/h1&gt;
&lt;p&gt;In C++, object-oriented programming is straightforward, which makes sense given it's the reason for C++'s existence. You might have a class declaration like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;class&lt;/span&gt; &lt;span class="n"&gt;Foo&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="nl"&gt;public:&lt;/span&gt;
  &lt;span class="n"&gt;Foo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;Foo&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="nl"&gt;private:&lt;/span&gt;
  &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and a definition like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Foo&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;Foo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;this&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;Foo&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;Foo&lt;/span&gt;&lt;span class="o"&gt;::~&lt;/span&gt;&lt;span class="n"&gt;Foo&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;delete&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(This is a bad example. Don't do this.)&lt;/p&gt;
&lt;p&gt;Now, in C, you don't have any of that fancy-shmancy constructor/destructor/assignment-operator/definite lifespan stuff. Instead you have functions! And pointers! And misery!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;typedef&lt;/span&gt; &lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
   &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="n"&gt;Foo&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;Foo_init&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Foo&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
   &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;// You fucked up&lt;/span&gt;
     &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
   &lt;span class="p"&gt;}&lt;/span&gt;
   &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;malloc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;sizeof&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
   &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;// Yes this happens&lt;/span&gt;
     &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
   &lt;span class="p"&gt;}&lt;/span&gt;
   &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
   &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;Foo_func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Foo&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;ret&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="n"&gt;ret&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;// Reconsider your life choices&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;ret&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;Foo_destroy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Foo&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;// No seriously, something&amp;#39;s wrong with you&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="n"&gt;free&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice all the differences:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No member functions. This one's actually pretty tame, as far as problems go, but it does mean you have to verify each and every time that your object &lt;em&gt;actually exists&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;No access control. I mean, sure, using a private implementation idiom might be a good idea, but then you have to constantly have one more layer of indirection, and that's just messy.&lt;/li&gt;
&lt;li&gt;No scope-based lifespan. Every time you want to use a &lt;code&gt;Foo&lt;/code&gt;, you have to manually call &lt;code&gt;Foo_init&lt;/code&gt; before using it, and &lt;code&gt;Foo_destroy&lt;/code&gt; after it leaves scope. If you don't, well, you might never know! It might still work. Or not. It depends on what's in the garbage memory you're writing to, which is a source of &lt;em&gt;most of the bugs in C and C++ programs&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;No real exceptions. Notice how every function that does something returns an &lt;code&gt;int&lt;/code&gt;? That's error handling. And it's all you've got. Sure, you can dress it up pretty by using Unix error codes or a custom enum, but at the core of it you're still passing a number around that says in some cryptic way whether or not your program is actually working, and you can't verify if anyone &lt;em&gt;is actually reading the value or just assuming it's okay&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Still with me? Good. We have eight more circles to pass through.&lt;/p&gt;
&lt;p&gt;Another feature present C++ provides you with, besides automatic lifespan and proper exception-handling (ish), is template programming. Suppose you had some kind of container object &lt;code&gt;Bar&lt;/code&gt;, which just held a single field of that type. (Don't ask why you'd want this. You wouldn't.) In C++, you could do this:&lt;/p&gt;
&lt;p&gt;template &lt;typename T&gt;
   class Bar {
     public:
       Bar(T t);
       T getItem();
     private:
       T item;
   };&lt;/p&gt;
&lt;p&gt;You could also do things like make &lt;code&gt;item&lt;/code&gt; a reference, make &lt;code&gt;getItem&lt;/code&gt; const, or a bunch of other options. If you wanted to do the same in C, you'd have to write something like this (skipping error-handling for now):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;Bar&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;Bar_init&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Bar&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="c1"&gt;// ...&lt;/span&gt;
  &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="c1"&gt;// ...&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;Bar_getItem&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Bar&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;ret&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="c1"&gt;// ...&lt;/span&gt;
  &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;ret&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
  &lt;span class="c1"&gt;// ...&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Welp, there goes type-safety. And scope-checking. If you didn't know for sure that &lt;code&gt;value&lt;/code&gt; and your &lt;code&gt;Bar&lt;/code&gt; would stay in the same scope, you'd have to do this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;Bar_init&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Bar&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;item_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="c1"&gt;// ...&lt;/span&gt;
  &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;malloc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item_size&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="c1"&gt;// ...&lt;/span&gt;
  &lt;span class="n"&gt;memcpy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;item_size&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="c1"&gt;//...&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;// Bar_getItem is the same for now&lt;/span&gt;

&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;Bar_destroy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Bar&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="c1"&gt;// ...&lt;/span&gt;
  &lt;span class="n"&gt;free&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="c1"&gt;// ...&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Oddly enough: this also shows the first steps along the way to type-checking: checking the &lt;em&gt;size&lt;/em&gt; of elements at runtime. It's more-or-less the only thing we can do, short of some twisted form of hell involving macros and cryptic compiler errors and drinking until you can forget you ever tried. Sure, this code is ugly, but it works. Most of the time. Onto the actual example...&lt;/p&gt;
&lt;h1&gt;Maps to Hell and Back Again&lt;/h1&gt;
&lt;p&gt;If you don't need iteration or deletion, a map has a pretty simple interface:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize with a set type of key and type of value (duh)&lt;/li&gt;
&lt;li&gt;put a value into the map for a specified key&lt;/li&gt;
&lt;li&gt;find a value (if it exists) put into the map for a given key&lt;/li&gt;
&lt;li&gt;destroy a map (also duh)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Written out in C, this might look something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;Map&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;Map_init&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;Map&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;key_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;value_size&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;Map_put&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;Map&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;Map_get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;Map&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;ret&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;Map_destroy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;Map&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Hey, notice the &lt;code&gt;key_size&lt;/code&gt; and &lt;code&gt;value_size&lt;/code&gt;? Remember what I said about type-checking? This is how you know how big your passed-in memory blocks will be. Or this is how big you &lt;em&gt;hope&lt;/em&gt; they'll be. (More on that later.)&lt;/p&gt;
&lt;p&gt;Sample usage (ignoring error checking for brevity) would be something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Map&lt;/span&gt; &lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;Map_init&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;sizeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;KEY_TYPE&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="k"&gt;sizeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;VALUE_TYPE&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;span class="n"&gt;KEY_TYPE&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="cm"&gt;/*...*/&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;VALUE_TYPE&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="cm"&gt;/*...*/&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;Map_put&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;VALUE_TYPE&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;ret&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;NULL&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;Map_get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;void&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;ret&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="cm"&gt;/* Do Other Stuff */&lt;/span&gt;
&lt;span class="n"&gt;Map_destroy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I'll skip the actual implementation for brevity's sake, but in our case I used a &lt;a href="http://en.wikipedia.org/wiki/Double_hashing"&gt;double-hashing&lt;/a&gt;-based open-addressing map using two hash functions based on Java's hashCode. (Good programming is lazy programming. If I'd taken this into account earlier, we wouldn't be writing C.) The primary reason you need &lt;code&gt;key_size&lt;/code&gt; and &lt;code&gt;value_size&lt;/code&gt; here is that both the keys and values are stored in an array, and you need to know how to index that array, and how big it should be. Adding a key to the map at an index boiled down to:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;memcpy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;key_array&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;key_size&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;key_size&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and adding a value for the key was very similar. Getting a value at an index turned out to be really simple as well:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;ret&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;value_array&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;value_size&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(This does introduce a bug later on. Try and guess how while you read the next few paragraphs.)&lt;/p&gt;
&lt;p&gt;If you want to make this threadsafe, you can even do that easily: since you're implementing it yourself, add a reader-writer lock, read-lock it on getting a value, and write-lock it when adding a key. (This also becomes important.)&lt;/p&gt;
&lt;p&gt;And that's it. Everything works. Maps are solved, and as &lt;a href="https://twitter.com/JacobAbes/status/488707294352654336"&gt;this tweet&lt;/a&gt; clearly shows, I've outperformed Bjarne himself at making a generic hash map. Close the shop, write the rest, and go home, right?&lt;/p&gt;
&lt;p&gt;Well, not exactly.&lt;/p&gt;
&lt;p&gt;Let's say you have a Map with key-type &lt;code&gt;K&lt;/code&gt; and value-type &lt;code&gt;V&lt;/code&gt;. Suppose, instead of passing a &lt;code&gt;V*&lt;/code&gt; to &lt;code&gt;Map_put&lt;/code&gt;, a client passes a &lt;code&gt;V**&lt;/code&gt;.  When you try to use the &lt;code&gt;V*&lt;/code&gt; later on (or possibly anything else in the map), something is bound to go &lt;em&gt;horribly wrong&lt;/em&gt;. So you think to yourself, "I want type-checking, but &lt;code&gt;void *&lt;/code&gt;s don't know how big they are. If only there were a way to know how big the original memory was..." And then you take another shot of vodka, because you realize that you need a function to act in local scope to the caller, and the only way to do this is macros.&lt;/p&gt;
&lt;p&gt;Let's start by changing some definitions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;Map_put&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;Map&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;key_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;value_size&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;Map_get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;Map&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;key_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;ret&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;size_t&lt;/span&gt; &lt;span class="n"&gt;ret_size&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The implementation changes are pretty simple: they check that &lt;code&gt;key_size&lt;/code&gt; and &lt;code&gt;value_size&lt;/code&gt; match those of the Map passed in, return an error if they don't, and then continue along in the way that the original implementation did. However, we can't rely on the user to actually use them right, because they were reckless enough to use a C-based map in the first place, so we add macros to call them:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#define MAP_PUT(m, key, value) Map_put(m, key, sizeof(*key), value, sizeof(*value))&lt;/span&gt;
&lt;span class="c"&gt;#define MAP_GET(m, key, ret) Map_get(m, key, sizeof(*key), &amp;amp;ret, sizeof(*ret))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We also pass in &lt;code&gt;ret&lt;/code&gt; as a single-indirection pointer instead, so we can actually retrieve the defined type. Doing a simple grep for the original functions and swapping them for their disfigured preprocessor cousins is quick if not painless, and bam: type-checking. It might be an ugly way to do it, but hell, it's the &lt;em&gt;only&lt;/em&gt; way to do it...short of some scary, using-macros-to-build-custom-types kind of solution that flashed through my mind the moment I said the other one was the only way. Excuse me, I need a cold shower.&lt;/p&gt;
&lt;p&gt;(Note: this obviously falls apart if &lt;code&gt;sizeof(T)&lt;/code&gt; and &lt;code&gt;sizeof(T*)&lt;/code&gt; are the same. Just don't store pointers or &lt;code&gt;int&lt;/code&gt;s.)&lt;/p&gt;
&lt;p&gt;Now that you've had 5 paragraphs to think about it, have you found the other bug? I'll give you a hint: it involves the paragraph &lt;em&gt;right after I mention it&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;See, thread-safety is good and all, but when you return &lt;em&gt;raw references to locations in the array&lt;/em&gt; as part of your API, it's pretty much useless. In a system where we had around 5 threads per client/server combination, and each thread potentially had a pointer to the &lt;em&gt;same&lt;/em&gt; Map, it wouldn't be inconceivable that they would all call &lt;code&gt;Map_get&lt;/code&gt; for the &lt;em&gt;same&lt;/em&gt; key. And then operate using the pointed-to value. And then try to manipulate it. All at once. And all the threads would go, "It's okay, I had the reader lock, so I can't be causing concurrency errors," calling &lt;code&gt;pthread_chortle&lt;/code&gt; and &lt;code&gt;pthread_grin_smugly&lt;/code&gt; as they look with disdain over the broken mess that they see the rest of the program to be, none the wiser that they are the bane of the client programmers, and the source of the "l'esprit de l'escalier"-esque frustration I experienced realizing my error long after the deadline had passed.&lt;/p&gt;
&lt;p&gt;Solving this problem would be pretty trivial (having a mutex for each key or value or index or whatever, adding a method to acquire and free the value for a key), but it just goes to remind one of the foolishness on which this endeavour is based. Programming threadsafe generics in C is best left to the most quixotic of programmers, who waste time tilting at windmills while there are &lt;em&gt;actual dragons to slay&lt;/em&gt;. To sum up the lessons learned:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you need a container type, consider C++ instead. If you decide to still use C, reconsider. Repeat &lt;em&gt;ad infinitum&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Concurrency is a fickle beast, but pointers tend to make things worse. &lt;/li&gt;
&lt;li&gt;This is mostly unrelated, but don't assume you can implement something just because you learned about it in class. I was reminded the hard way that a fixed-length closed-hashing hash-map needed to have a prime size to ensure it's actually filled. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To all the people that called us idiots for doing this...well, you had a point. Still, I like to think that I gained something from all those hours spent staring at pointers and structs to find the &lt;em&gt;one&lt;/em&gt; function where the wrong variable was passed by value. I mean, I don't know what it was that I gained. &lt;/p&gt;
&lt;p&gt;I'll just have to dereference it and find out.&lt;/p&gt;
&lt;p&gt;J&lt;/p&gt;</summary><category term="nerdish"></category><category term="hubris"></category><category term="don't try this at home"></category></entry><entry><title>My Dinner with Java</title><link href="http://jabrahams.ca/my-dinner-with-java.html" rel="alternate"></link><updated>2014-04-11T00:00:00Z</updated><author><name>Jacob Abrahams</name></author><id>tag:jabrahams.ca,2014-04-11:my-dinner-with-java.html</id><summary type="html">&lt;p&gt;I was chatting online with some friends from school, when the topic of programming came up. This wasn't much of a surprise, since we all do this for a living, but it did lead to a discussion on Java. I've made my stance on Java &lt;a href="/java.html"&gt;pretty clear&lt;/a&gt;, but &lt;a href="http://www.gurjantkalsi.com/"&gt;Kalsi&lt;/a&gt; made an interesting point on  the topic:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I don't even think Java is a bad language. It's like a spoon: you can pretty much use a spoon as any other utensil. And people do. And it makes me mad. But I'm not mad at the people who make spoons [...] nor do I think spoons are shitty utensils.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We then went off on a tangent to apply this to pretty much everything else we could think of, and I figured I'd share the fruits of our labour in this article. Some of what follows originates from Kalsi, some from my part of that discussion, and some just flowed out while I was writing this article.&lt;/p&gt;
&lt;p&gt;(I've excluded an entry on Ruby on Rails for now, since it's off-topic.)&lt;/p&gt;
&lt;h2&gt;.NET&lt;/h2&gt;
&lt;p&gt;You don't actually have any utensils. Your weird roommate, however, has a drawer full of silverware, which he gives to you whenever you ask. Honestly, it's pretty nice stuff, but you have no ownership over it, and your roommate's pretty picky about always putting it back when you're done with it. He's also kind of odd about his drawer, and you're never sure that you aren't going to wake up someday and find half of the drawer missing, or all of the spoons replaced with shovels, or all of the knives suddenly double-ended.&lt;/p&gt;
&lt;p&gt;Maybe you should just move out.&lt;/p&gt;
&lt;h2&gt;Assembly&lt;/h2&gt;
&lt;p&gt;You're eating with a revolver instead of a spoon. "It's faster," you claim to your incredulous colleagues, but really you just get off on the thrill of nearly scattering your brainmass on the wall every time you eat soup. You're always concentrating so hard on your keeping a steady but light grip that you hardly notice if you're eating pasta or your pet gila moster, Hernandez, who admittedly had it coming, since he should have figured out to stay away from you &lt;em&gt;like everyone else&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;C&lt;/h2&gt;
&lt;p&gt;Instead of forks and knives, you bought a surplus of wood blocks and a whittling knife. The first time you made soup, you ended up with a mouthful of splinters and an empty stomach. However, you're definitely learning, and it's been &lt;em&gt;hours&lt;/em&gt; since the last time you got a splinter from one of your homemade utensils. You've gotten good at whittling, and you kind of have to, unless you want to accidentally carve a hole in the side of your face.&lt;/p&gt;
&lt;h2&gt;C++&lt;/h2&gt;
&lt;p&gt;You've got a hunting knife and a carving fork. About as good for prison fights as steak dinners, you know to use other tools in circumstances befitting them (or at least you do since the accident). The whittling knife still sits in the corner, but you haven't touched it in months, and you're kind of glad for that since the nightmares haven't subsided yet. The fork and knife also came with a darkened box that supposedly contains a single spoon, but every time you eat something new you get a funny feeling that it's a &lt;em&gt;different spoon&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Javascript&lt;/h2&gt;
&lt;p&gt;Instead of a fork or spoon, you get some vague "Utensil" that looks like it came from one of the other boxes, yet claims to be all-purpose. Every time you change your Utensil in any way, you change &lt;em&gt;every other Utensil in existence&lt;/em&gt;. This wouldn't be so bad, but other people aren't as good at taking care of and cleaning their Utensils, so yours gets grungy and bogged down at unpredictable (but incessant) intervals. Some guy came along one day and decided to "upgrade your Utensil," but really he just added a bunch of unnecessary and poorly-designed features on what was supposed to be a simple way to eat soup. You can't even &lt;em&gt;drink&lt;/em&gt; things reliably now, since it's been modded to act like a straw. Still, it's the most convenient solution available, so you grumble along until the day it inevitably develops sentience and decides to murder you.&lt;/p&gt;
&lt;h2&gt;PHP&lt;/h2&gt;
&lt;p&gt;PHP is eating with your hands. You'll get the job done (most of the time), but you're going to make a mess.&lt;/p&gt;
&lt;h2&gt;Python&lt;/h2&gt;
&lt;p&gt;Python is a glass fork. Yes, it's pretty. Yes, it's fairly versatile, you can use it for anything you want and it will probably be okay, but it won't be &lt;em&gt;fast&lt;/em&gt;, because the faster it's used the more likely it is to splinter and crack. Sure, it will continue to look pretty, but that will become less and less important as it cuts more tiny holes into your mouth and digestive tract. It also &lt;em&gt;really&lt;/em&gt; shouldn't be used for something big, since that's just asking for it to immediately shiv you in the lips.&lt;/p&gt;
&lt;h2&gt;Scala&lt;/h2&gt;
&lt;p&gt;A box comes in the mail with a spoon and a 3D-Printer. At first you use the spoon, because it's what you're used to and the printer keeps making funny noises. Over time, though, you learn to print a fork and a knife, then different-sized variants for different foods. Then chopsticks and electric carving knives. At some point you start making utensils that look like nothing the world has seen before, things you couldn't put into words if you tried. After days, you produce an object that resembles a fork at first glance but, when held at 72 degrees to the ground, launches its contents toward the nearest wall at Mach 3.&lt;/p&gt;
&lt;p&gt;(This may be in progress, if it ever updates again. If not, it's not)&lt;/p&gt;
&lt;p&gt;-J&lt;/p&gt;</summary><category term="nerdish"></category><category term="programming"></category></entry><entry><title>C++ - Template Hell</title><link href="http://jabrahams.ca/cpp-templates.html" rel="alternate"></link><updated>2014-03-31T00:00:00Z</updated><author><name>Jacob Abrahams</name></author><id>tag:jabrahams.ca,2014-03-31:cpp-templates.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Part II in a &lt;a href="cpp-casts.html"&gt;series&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let's talk about generic programming.&lt;/p&gt;
&lt;p&gt;In normal, static-type programming, you say things like, "I know &lt;code&gt;foo&lt;/code&gt; is an &lt;code&gt;A&lt;/code&gt;, and &lt;code&gt;bar&lt;/code&gt; is a &lt;code&gt;B&lt;/code&gt;, so I know what type I'll get when I write &lt;code&gt;foo + bar&lt;/code&gt;." This is pretty efficient: you know what type everything will be, the compiler does too, and if you're wrong there's a clear and efficient way of telling you so. (ie. The compiler can just go and say, "type &lt;code&gt;A&lt;/code&gt; is not type &lt;code&gt;Q&lt;/code&gt;," and that will be that.)&lt;/p&gt;
&lt;p&gt;However, suppose you want to say, "I know (or believe) &lt;code&gt;foo&lt;/code&gt; has some member &lt;code&gt;baz&lt;/code&gt;, and &lt;code&gt;bar&lt;/code&gt; has some members &lt;code&gt;twiddle&lt;/code&gt; and &lt;code&gt;fiddle&lt;/code&gt;, but I don't know or care what type they actually are. Let me write a function that takes &lt;code&gt;foo&lt;/code&gt; and &lt;code&gt;bar&lt;/code&gt; and does something useful." In languages like C, you can...write something else, because C doesn't let you do anything about it. &lt;code&gt;A&lt;/code&gt; is &lt;code&gt;A&lt;/code&gt;, and nothing else is &lt;code&gt;A&lt;/code&gt;, no matter how much it dreams of being &lt;code&gt;A&lt;/code&gt; at night. (Unless you do some crazy type-punning. Don't do that). In other, more developed OO languages, you have (up to) three solutions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Interface-based programming, or thinking far-enough ahead and giving your objects a relevant base type to do what you want them to. This is nice when possible, but may have problems when you can't modify the interface after-the-fact. (In these cases the &lt;a href="http://en.wikipedia.org/wiki/Adapter_pattern"&gt;Adapter design pattern&lt;/a&gt; may help.) Languages with interfaces also often have some kind of generics to apply the concept to containers and the like.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Duck-typing. This basically says, "Screw type-safety, figure it out when you try to run the code. If &lt;code&gt;foo&lt;/code&gt; doesn't have a &lt;code&gt;baz&lt;/code&gt;, deal with it." This is extremely common in scripting languages, and the source of endless headaches and bugs that I won't go into here. (I have to save &lt;em&gt;something&lt;/em&gt; for my Python rant.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Templates.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Templates are a solution to generic programming the same way a hammer is a solution to a Rubik's Cube. Take the following function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Some_Return_Type&lt;/span&gt; &lt;span class="nf"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Some_Input_Type&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
   &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;blargle&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With some clever definitions, we can actually make this compile:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kr"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;TIn&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kr"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;TRet&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="n"&gt;TRet&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TIn&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;blargle&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can do even better in C++11:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kr"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;decltype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;blargle&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;blargle&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And even better in C++14:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kr"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="k"&gt;auto&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;blargle&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Looks kind of like duck-typing, right? Sure...if duck-typing generated more code every time you called the function with a different set of types. Further, the header-source model of C++ becomes &lt;em&gt;impossible&lt;/em&gt; with templates, since the compiler needs the definition (and not just the declaration) to see if it will compile or not. But, you know what? None of this is that bad. You could argue this is actually a good way of doing things and, on a good day, I might agree with you. No, let's go where it actually gets fucked up.&lt;/p&gt;
&lt;h2&gt;Numerical Templates&lt;/h2&gt;
&lt;p&gt;If you're a Java guy or girl (and I know you are), you're probably going, "Oh neat, templates are just like generics with less obvious error messages." And that means you've never seen templates on anything else, like, say, numbers.&lt;/p&gt;
&lt;p&gt;Please pick up your jaw. It doesn't belong on the floor.&lt;/p&gt;
&lt;p&gt;Sure, instead of &lt;code&gt;typename&lt;/code&gt;, you can put &lt;code&gt;size_t&lt;/code&gt; or &lt;code&gt;bool&lt;/code&gt; or even &lt;code&gt;MyMessedUpEnumType&lt;/code&gt; inside the angle brackets. Ostensibly for fixed-size arrays (or something?), numerical templates have a not-so-nice unintended feature: template metaprogramming. As in, you can write programs inside template declarations. Neat, sure, but unclear as &lt;em&gt;hell&lt;/em&gt;. For instance, here's a way of computing factorials of compile-time constants:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;size_t&lt;/span&gt; &lt;span class="nb"&gt;N&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="nx"&gt;struct&lt;/span&gt; &lt;span class="nx"&gt;Fact&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nx"&gt;static&lt;/span&gt; &lt;span class="nx"&gt;size_t&lt;/span&gt; &lt;span class="nb"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;
&lt;span class="nx"&gt;template&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;size_t&lt;/span&gt; &lt;span class="nb"&gt;N&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="nx"&gt;size_t&lt;/span&gt; &lt;span class="nx"&gt;Fact&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nb"&gt;N&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="nl"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;N&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;Fact&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nb"&gt;N&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="nl"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="nx"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;gt;&lt;/span&gt;
&lt;span class="nx"&gt;size_t&lt;/span&gt; &lt;span class="nx"&gt;Fact&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="nl"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Put &lt;code&gt;Fact&amp;lt;5&amp;gt;&lt;/code&gt; in your program, and it'll just be replaced by 120 seamlessly. Put &lt;code&gt;Fact&amp;lt;7000&amp;gt;&lt;/code&gt; and, well, it might compile. Eventually. (GCC does not compile, it just grumbles about "maximum template depths" and mutters curses from the Old World under its breath.)&lt;/p&gt;
&lt;p&gt;The great thing about numerical templates is that they play some part in making compiler errors &lt;em&gt;completely unreadable&lt;/em&gt;. The compiler doesn't know you. It can't assume you won't template on an enumeration. It doesn't know your life and what you've been through. GCC can't afford to take you out to dinner and learn your favourite band. Hell, you might've even &lt;em&gt;wanted&lt;/em&gt; the template not to compile...&lt;/p&gt;
&lt;h2&gt;SFINAE and type-traits&lt;/h2&gt;
&lt;p&gt;C++ has this neat (read: horrifying) idiom called, "Substitution Failure is not an Error," or SFINAE for short. It basically says to the compiler, "I've defined this template in 2 (or more) ways. Figure out which one I mean." While (ostensibly) included for good reasons (for instance, you might want to handle pointer-types and value-types differently), it's usually used in ways that fit more into dynamically typed languages: determining attributes of types after-the-fact. Which, y'know, &lt;em&gt;defeats the original purpose of templates&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;But you know what? Compile-time duck-typing isn't the worst solution. It's strictly better than &lt;em&gt;runtime&lt;/em&gt; duck-typing, even if you have to know medieval Cyrillic to understand why the compiler thinks you're wrong when you are. It's also more flexible (&lt;em&gt;twitch&lt;/em&gt;) than Java-style generics in a lot of ways. Lastly, you have to remember who you're dealing with: C++ developers will run wild with &lt;em&gt;whatever feature you give them&lt;/em&gt;. A way to make the compiler Turing-complete? That's a small price to pay.&lt;/p&gt;
&lt;p&gt;And at least you aren't paying at runtime.&lt;/p&gt;
&lt;p&gt;J&lt;/p&gt;</summary><category term="tech"></category><category term="c++"></category><category term="nerdish"></category></entry><entry><title>Cast operators in C++</title><link href="http://jabrahams.ca/cpp-casts.html" rel="alternate"></link><updated>2014-03-17T12:00:00Z</updated><author><name>Jacob Abrahams</name></author><id>tag:jabrahams.ca,2014-03-17:cpp-casts.html</id><summary type="html">&lt;p&gt;Alright, I'm done bitching about Java. The maelstrom from my last post was exhausting (on both sides of the issue), and I don't have enough energy to deal with that every single week. Plus, I'm basically out of content on Java (more or less). Originally, that meant that I would spend this week on Python. However, I saw that as kicking another anthill altogether, so I'm pushing that one back at least a week.&lt;/p&gt;
&lt;p&gt;Instead, let's talk about C++.&lt;/p&gt;
&lt;p&gt;As you may have noticed, the title of this article is &lt;em&gt;somewhat&lt;/em&gt; narrow in scope. That's mostly because I couldn't compress my C++ critique to a single article, and slightly to abuse the good faith of my current readerbase in a desperate bid to draw out whatever lifespan this blog has to the last possible second. It's also because I work with C++ fairly rarely, so writing these articles requires drawing out stale vitrol from the dank recesses of my mind, which takes time to cultivate into proper rage. Plus, my relationship with C++ is so much less hateful: I have on multiple occasions &lt;em&gt;actively chosen&lt;/em&gt; to use C++ for tasks. So this will be the first of (let's say) 3 articles. (This number will not update with the number of articles I write. It will just become wrong.)&lt;/p&gt;
&lt;h2&gt;Brief History&lt;/h2&gt;
&lt;p&gt;I'm assuming my audience is mostly made up of classmates or colleagues, but it doesn't hurt to go over how C++ came to be. Around the early 70s, Dennis Ritchie was building Unix at Bell Labs, when he decided he needed a language that was more portable, more readable, and less error-prone than assembly code. He built C instead, and the programmers of the 70s took to it like the other young adults of the 70s took to recreational drug use. By the 80s, Bjarne Stroustrup saw the drug addiction-like damage C was doing to people, and decided that the fix of the problem would be to &lt;em&gt;add more features&lt;/em&gt;, which is eerily similar to heroin's early history. He spearheaded efforts to build a new language, which he dubbed, "C with Classes, Exceptions, Metaprogramming, Generics, Algorithms, and everything else I could think of." This was shortened to "C with Classes," which was later shortened to "C++" because languages with long names are rarely successful. [citation needed] &lt;/p&gt;
&lt;p&gt;(Side note: because of post-increment semantics, Bjarne made a language that was the same as what C &lt;em&gt;used&lt;/em&gt; to be, and changed C in the process. The C++-influenced changes in C since the 80s attest to this.)&lt;/p&gt;
&lt;h2&gt;Cast Operators&lt;/h2&gt;
&lt;p&gt;Suppose you have two types, &lt;code&gt;T&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt;, and an instance of each, &lt;code&gt;t&lt;/code&gt; and &lt;code&gt;v&lt;/code&gt; respectively. In C, converting a &lt;code&gt;T&lt;/code&gt; to a &lt;code&gt;V&lt;/code&gt; is easy: just write &lt;code&gt;v = (V)t&lt;/code&gt; and it'll probably do something that makes sense. (To elaborate, it will either make a logical conversion if &lt;code&gt;T&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; are both numerical types, and it will just reinterpret the value at that point in memory without changing it otherwise. Or it will crash in an implementation-defined way if you're being an idiot, and you deserve that.)&lt;/p&gt;
&lt;p&gt;C++ decided that C's handling of the situation was, like the rest of the language, insufficient, and went about fixing it in an extremely questionable way, just like they did with the rest of the language. They noted a few problems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Casts are too easy to hide in code. If you're looking for places where &lt;code&gt;T&lt;/code&gt;s are casted to &lt;code&gt;V&lt;/code&gt;s, what string are you going to search the code for? &lt;code&gt;(V)&lt;/code&gt;? Good luck with that.&lt;/li&gt;
&lt;li&gt;Casts don't show any intent. What if you thought that &lt;code&gt;T&lt;/code&gt;s have a logical interpretation as &lt;code&gt;V&lt;/code&gt;s, but in fact your code just ends up manipulating parts of &lt;code&gt;V&lt;/code&gt;s that you really shouldn't be touching? To the C compiler, you could &lt;em&gt;want&lt;/em&gt; to access parts of &lt;code&gt;V&lt;/code&gt; like that, because that's also something C-programmers do, and it has no reason to assume you're a sensible human being.&lt;/li&gt;
&lt;li&gt;Casts aren't user-definable. Well, they kind of are, if you assume that your &lt;code&gt;struct&lt;/code&gt;s are always packed in the same way across different architectures and operating systems. (See above for why the compiler doesn't assume you aren't doing this.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To fix these perceived problems, C++ did a bunch of things. First and foremost, they created four cast operators:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;static_cast&lt;/code&gt; - you &lt;em&gt;know&lt;/em&gt; that &lt;code&gt;t&lt;/code&gt; can be thought of as a &lt;code&gt;V&lt;/code&gt;, or you're willing to pay some pretty hefty consequences if you're wrong.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dynamic_cast&lt;/code&gt; - &lt;code&gt;T&lt;/code&gt; and &lt;code&gt;V&lt;/code&gt; are pointer/reference-types, and &lt;code&gt;T&lt;/code&gt; is a base type of &lt;code&gt;V&lt;/code&gt;. You're pretty sure &lt;code&gt;t&lt;/code&gt; is actually a &lt;code&gt;V&lt;/code&gt;, but you don't want your program to explode if you're wrong, so you're okay if an exception (specifically &lt;code&gt;std::bad_cast&lt;/code&gt;) is thrown when your assumption turns out to be false. (If everything is pointers, &lt;code&gt;NULL&lt;/code&gt;/&lt;code&gt;nullptr&lt;/code&gt; is returned instead.)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;reinterpret_cast&lt;/code&gt; - you know that &lt;code&gt;t&lt;/code&gt; isn't a &lt;code&gt;V&lt;/code&gt;, but you think that the binary data in a &lt;code&gt;T&lt;/code&gt; makes sense interpreted as a &lt;code&gt;V&lt;/code&gt;, so you want the compiler to just look the other way for a second or two.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;const_cast&lt;/code&gt; - You know that &lt;code&gt;T&lt;/code&gt;s say they shouldn't be modified, but you have reason to believe that it's okay to do so (assuming that &lt;code&gt;T&lt;/code&gt; is a &lt;code&gt;const V&lt;/code&gt;). You can also cast away volatility, but I don't really know why you would.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(Boost is also trying to add &lt;code&gt;lexical_cast&lt;/code&gt; to the group, which would act like &lt;code&gt;[tT]oString&lt;/code&gt; in Java/C# but in a bidirectional way, which is kind of neat.)&lt;/p&gt;
&lt;p&gt;If you didn't catch my tone, &lt;em&gt;most of these casts are almost always wrong&lt;/em&gt;. Let's go through them again:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;static&lt;/code&gt; is fine, generally speaking. Nice and greppable, clearly shows intent, and is comparable in speed to a C-cast.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dynamic&lt;/code&gt; shows too much doubt on the part of the programmer: why do you only &lt;em&gt;think&lt;/em&gt; &lt;code&gt;t&lt;/code&gt; is a &lt;code&gt;V&lt;/code&gt;? Where is this object coming from if you can't accurately describe its type &lt;em&gt;but you still need to know what it is&lt;/em&gt;? Most importantly, &lt;em&gt;why are you ever downcasting&lt;/em&gt;? (It's also really slow compared to the others.)&lt;/li&gt;
&lt;li&gt;Do I really need to explain why &lt;code&gt;reinterpret&lt;/code&gt; is bad? If &lt;code&gt;static&lt;/code&gt; won't compile, then you should step back, take a deep breath, and go back to writing assembly. A &lt;code&gt;std::pair&amp;lt;int, int&amp;gt;&lt;/code&gt; is not and will never be a &lt;code&gt;double&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;const&lt;/code&gt; is justifiable when you're dealing with stupid APIs that give you &lt;code&gt;const&lt;/code&gt; variables that you know you can actually write to. If you're &lt;code&gt;const_cast&lt;/code&gt;ing in code entirely written by you or someone you know, you should &lt;em&gt;really&lt;/em&gt; reconsider.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For backwards-compatability, C++ includes the C-style cast operator. It's behaviour is basically to run through all these cast operators until one makes sense. This is, of course, almost never what you want, since it solves &lt;em&gt;none&lt;/em&gt; of the problems the operator had in C.&lt;/p&gt;
&lt;p&gt;Oh, and there are two other cast operator-like options provided in C++:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;you can define an &lt;code&gt;operator V&lt;/code&gt; function inside &lt;code&gt;T&lt;/code&gt;'s definition. Or anywhere else, because operators can be overloaded in any scope.&lt;/li&gt;
&lt;li&gt;you can define a constructor in &lt;code&gt;V&lt;/code&gt; that takes a single argument that is implicitly convertible to a &lt;code&gt;T&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, I'm leaving something out here. See, both of these are &lt;em&gt;implicit&lt;/em&gt;, meaning you can just write &lt;code&gt;v = t;&lt;/code&gt; and it will just pick the first thing that works. Sure, you can declare either as &lt;code&gt;explicit&lt;/code&gt;, but only in C++11-onwards: &lt;code&gt;explicit&lt;/code&gt; only worked for single-argument constructors &lt;em&gt;for at least 15 years&lt;/em&gt;. This lead to ridiculous efforts to get around the language's limitation including (but not limited to) the &lt;a href="http://www.artima.com/cppsource/safebool.html"&gt;safe-bool idiom&lt;/a&gt; for pointer-like truth-value detection. (This is one of my favourite instances of an extremely complex solution to a simple problem.) Thankfully, this kind of innovation is no longer necessary, but it's still a "gotcha" to watch for, since so few other languages mark conversions as implicit by default.&lt;/p&gt;
&lt;p&gt;(I'm not even going to &lt;em&gt;begin&lt;/em&gt; on how complicated this gets with templates and determining function prototypes, because this article is long enough and I want to do templates as a separate article.)&lt;/p&gt;
&lt;p&gt;Does C++ handle this all wrong? I'm on the fence with that. Sure, languages like C# have a more structured, logical approach to casts and conversions (eg. limiting the scope, forcing an explicit/implicit declaration etc.), but C++'s design has always been about giving the programmer as many tools as possible without restricting their freedom. Really, it should be common sense not to abuse &lt;code&gt;reinterpret_cast&lt;/code&gt;, but if you're crazy enough to want to, well, C++ will hand you the keyword. It's not going to stop you from pointing the gun at your foot, but it'll help you dial 911. (Unfortunately, it won't stop you from tearing out the phone lines.) The implicit-cast-by-default thing is strange, sure, but once you're aware of it you can probably handle it. All in all, C++ lets you do what you want quickly and (if you choose to) clearly, and that's all that really matters.&lt;/p&gt;
&lt;p&gt;To sum up:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C++ adds 4 more cast operators. 3 are usually a sign of bad programming somewhere along the line.&lt;/li&gt;
&lt;li&gt;C++ lets you define conversions, but you might be surprised when it decides to apply them.&lt;/li&gt;
&lt;li&gt;The design of casting in C++ makes sense for C++, as long as you're okay with that&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;C++ is &lt;em&gt;huge&lt;/em&gt;. I just wrote more on one tiny part of it than I wrote on &lt;em&gt;all of Java&lt;/em&gt;. And I'm probably going to write more, given the right feedback. If that sounds like a good thing, let me know. 'Til then, I'll just continue waiting for my Java-based IDE to unfreeze.&lt;/p&gt;
&lt;p&gt;(Seriously. Not even kidding)&lt;/p&gt;
&lt;p&gt;J&lt;/p&gt;</summary><category term="nerdish"></category><category term="programming"></category><category term="c++"></category></entry><entry><title>Java - the Barbed-Wire Fence of Programming</title><link href="http://jabrahams.ca/java.html" rel="alternate"></link><updated>2014-03-08T19:00:00Z</updated><author><name>Jacob Abrahams</name></author><id>tag:jabrahams.ca,2014-03-08:java.html</id><summary type="html">&lt;p&gt;Occasionally, I like to take a step back and look at the programming languages I've used over the years. What better place to start than with my first, my last, my everything?&lt;/p&gt;
&lt;p&gt;I've been programming Java since high school, and it was the first real language I was exposed to. I suspect that, when I sleep, I still type&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;because it's &lt;em&gt;that&lt;/em&gt; ingrained in my muscle memory. And you know what?&lt;/p&gt;
&lt;p&gt;Java is an &lt;em&gt;awful&lt;/em&gt; choice for a first language.&lt;/p&gt;
&lt;p&gt;I'm not going to just state that, I'll give reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Java &lt;em&gt;requires&lt;/em&gt; Object-Oriented Programming. Every java program is a function embedded in an object. Way to keep a gentle learning curve, there.&lt;/li&gt;
&lt;li&gt;Java relies extensively on imports from other libraries, and other classes and types. That's so much to explain to a beginning programmer. Or not, which was the case with me: just don't explain anything, and just use it.&lt;/li&gt;
&lt;li&gt;Java forces exception handling. Never mind that you're just starting out and probably don't even know what an exception is.&lt;/li&gt;
&lt;li&gt;Java is garbage-collected, which means that you don't even know where things are going. &lt;code&gt;new&lt;/code&gt; is just a keyword that makes things happen to you.&lt;/li&gt;
&lt;li&gt;Java tends to use verbosity to the extreme: it makes simple tasks tedious, and complex tasks impossible.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Probably think with Point 4 up there that I advocate C or C++ instead, right? Well, yes and no. I think that those languages are great for teaching a fledgling programmer that the program you write can do horrible, terrifying things if you fuck up, which can be great for traumatizing a freshman programmer into getting his/her shit together, or it can mess up their mind really badly. I also think that languages like Python are too high-level to make certain the programmer knows exactly what they're doing, relying on language constructs more than common sense. So, I'd actually like to see someone teach using C and Python &lt;em&gt;in combination&lt;/em&gt;. (Though not in CPython, because that's really just the worst of both worlds.)&lt;/p&gt;
&lt;p&gt;Anyway, this article isn't about ripping on Java as a choice of teaching language. It's about ripping on Java's design choices. Full disclosure from this point on: I actually write Java full-time, so I'm going to avoid writing my way out of a job.&lt;/p&gt;
&lt;p&gt;Let's get down to it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verbosity. Oh my &lt;em&gt;God&lt;/em&gt; the verbosity. My fingers sometimes hurt after typing Java more than they'd hurt after an essay or a particularly long-winded blog post.&lt;/li&gt;
&lt;li&gt;Strictness in file organization. A file named [Foo].java can only have one public class named [Foo]. If it's in a packaged called com.abba.jesus.pidgeot, then it's file path has to be &lt;code&gt;[something]/com/abba/jesus/pidgeot/Foo.java&lt;/code&gt;, even if there's nothing in packages com, com.abba, or com.jesus.&lt;/li&gt;
&lt;li&gt;Weird type-boxing/casting rules. Each of the value types is implicitly convertible to a related class in &lt;code&gt;java.util&lt;/code&gt; (ie. &lt;code&gt;int&lt;/code&gt; =&amp;gt; &lt;code&gt;java.util.Integer&lt;/code&gt;)...except as an argument to a generic class, where it has to be declared as the class rather than the value-type (ie. &lt;code&gt;List&amp;lt;int&amp;gt;&lt;/code&gt; is a ParseException). This is fine (mostly), except that any function that takes a generic &lt;code&gt;T[]&lt;/code&gt; has to have a &lt;em&gt;separate implementation for each value type&lt;/em&gt;. Further, Java disallows casts between different class types (that aren't covariant)...except for a variable of &lt;em&gt;any&lt;/em&gt; type &lt;code&gt;v&lt;/code&gt;, &lt;code&gt;v + ""&lt;/code&gt; is &lt;em&gt;always&lt;/em&gt; a string and never results in a NullPointerException when &lt;code&gt;v&lt;/code&gt; is &lt;code&gt;null&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The weirdness of exception handling. Prior to Java 7, there was no way to handle multiple exceptions in a single block, leaving copious room for typos in copy-pasting the same damn block to both related handlers. Oh, and you have to handle every Exception, regardless of if it makes sense to...unless it's a RuntimeException. Then the JVM gives it a free pass. Why even &lt;em&gt;have&lt;/em&gt; a duality? Why not just have stuff like that be handled as Errors or something?&lt;/li&gt;
&lt;li&gt;Weirdly inconsistent library support. See &lt;a href="http://tech.puredanger.com/2010/03/31/do-we-want-a-java-util-pair/"&gt;this article&lt;/a&gt; on the concept of a Pair class, for instance. They basically say, "Oh I don't think it's necessary, so no one else will." I've also heard people say, "Programmers will use it wrong." That is a &lt;em&gt;terrible&lt;/em&gt; reason not to support something, especially in a util class of all things. On that note...&lt;/li&gt;
&lt;li&gt;Complete lack of operator overloading. Which would you rather see:
    &lt;pre&gt;
    BigInteger a = new BigInteger("12343212");
    BigInteger b = new BigInteger("234231999");
    BigInteger c = new BigInteger("65565790099");
    return c.add(b.muliply(a));
    &lt;/pre&gt;
    or
    &lt;pre&gt;
    return c + (b * a);
    &lt;/pre&gt;
    Again, the response seems to be, "People will use it wrong." And a lot of the time, the hatred is directed at C++, specifically the stream operator:
    &lt;pre&gt;
    int i;
    cin &amp;gt;&amp;gt; i;
    cout &amp;lt;&amp;lt; i &amp;lt;&amp;lt; endl;
    &lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To compare, let's discuss Scala, a language built on top of the JVM. Let's see what it does differently:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verbosity. Scala tends towards the succinct whenever possible, at least in my experience.&lt;/li&gt;
&lt;li&gt;More free-form package organization. Scala will find your files, don't worry about what you name them. You can also put more than one top-level public class in a single file (though this is still discouraged)&lt;/li&gt;
&lt;li&gt;More intuitive type-coercion. Scala's type system is complex. I don't know enough about lambda calculus to pretend I fully understand every part of it, but I do know they make value types &lt;em&gt;effectively just a different part of the standard hierarchy&lt;/em&gt;. &lt;code&gt;List&amp;lt;int&amp;gt;&lt;/code&gt; isn't a syntax problem anymore: it just works.&lt;/li&gt;
&lt;li&gt;Free-form casting and conversions. Want &lt;code&gt;A&lt;/code&gt; to always make sense as a &lt;code&gt;B&lt;/code&gt;? Why not! You can do that.&lt;/li&gt;
&lt;li&gt;No exceptions are forced to be checked anymore. Rejoice!&lt;/li&gt;
&lt;li&gt;On the library support, it just tries to have as wide support as possible, which isn't always a good thing. (Want to know all of Scala? Too bad, that's what the internet's for.)&lt;/li&gt;
&lt;li&gt;Operator overloading is just another function. This &lt;em&gt;maybe&lt;/em&gt; goes a bit too far, since you can make operators using basically any character. (The &lt;code&gt;/:&lt;/code&gt; operator for fold-left is my favourite.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And, while we're at it, let's look at C#, a language made because Microsoft didn't want to be sued for using Java at one point[citation needed]:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Still verbose, though I'd argue less so.&lt;/li&gt;
&lt;li&gt;Package organization? Totally up to you. Hey, have 5 classes per file if you want.&lt;/li&gt;
&lt;li&gt;A similar (though simpler) type system to Scala. Value types are &lt;code&gt;struct&lt;/code&gt;s, everything else is an &lt;code&gt;obejct&lt;/code&gt;, and &lt;code&gt;struct&lt;/code&gt;s can be semantically treated like &lt;code&gt;object&lt;/code&gt;s.&lt;/li&gt;
&lt;li&gt;Define your own cast operators! Make them as explicit as you want! &lt;/li&gt;
&lt;li&gt;Exceptions don't need to be checked. Funny how often this pops up...&lt;/li&gt;
&lt;li&gt;A decent-ish standard library which supports most of the things you'd want. Maps that &lt;em&gt;actually&lt;/em&gt; act the same as other iterables (as they damn-well should).&lt;/li&gt;
&lt;li&gt;Override &lt;em&gt;only the existing operators and only within the definition  of the class itself&lt;/em&gt; (ie, no overriding &lt;code&gt;int + int&lt;/code&gt; or something). Humourously, the bit shift operators (ie. the straming operators in C++) can only have the second operand be an &lt;code&gt;int&lt;/code&gt;, &lt;em&gt;because they're still pissed about that&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Don't take this to mean Java does everything wrong. The JVM is a great piece of technology, and the language has some neat constructs seen rarely elsewhere. (Anonymous interfaces, anyone?) But &lt;em&gt;so many&lt;/em&gt; of its design decisions serve merely to act out of spite to the admittedly-overindulgent amount of freedom that C++ gives programmers. And spite is a &lt;em&gt;terrible&lt;/em&gt; way to drive a design process. Scala, it seems, was designed out of counter-spite: it is &lt;em&gt;so&lt;/em&gt; different from Java, yet is based on the exact same technology stack. In my mind, &lt;em&gt;Scala is the biggest "Fuck You" to Java that exists&lt;/em&gt;. (C# only loses out on this because it's tied so tightly to Windows that it rarely sees the light of day outside of Microsoft developer pet-projects and ASP.NET-based websites.) And when a competitor springs up just to subvert everything you've done in an ironic way? Face it, Java, you've become the new C++. And not in a good way.&lt;/p&gt;
&lt;p&gt;So there's my Java rant. It's been a long time coming. I'll probably dissect Python at some point, so if that interests you, tune in again.
'Til then, I'll be waiting for my code to compile.&lt;/p&gt;
&lt;p&gt;J&lt;/p&gt;</summary><category term="tech"></category><category term="programming"></category><category term="nerdish"></category></entry><entry><title>When Failsafes Fail</title><link href="http://jabrahams.ca/failsafes.html" rel="alternate"></link><updated>2014-03-08T00:00:00Z</updated><author><name>Jacob Abrahams</name></author><id>tag:jabrahams.ca,2014-03-08:failsafes.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Disclaimer: this post is heavily personal, and not exactly cheery. It's also probably the most honest thing I will ever post on this blog, and easily the least emotionally-restrained. It has a lot of not-well-known info about me and my life that I've never been comfortable sharing. Depending on who you are and how well you know me, it may change the tenor of our interaction, and not necessarily for the better. Stop here if you aren't okay with that, or think I wouldn't be.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Still reading? Alright, you've been warned.&lt;/p&gt;
&lt;p&gt;This post is about the effects and truths of depression and related mental illnesses, from a first- and second-hand perspective. I'll start by going over why I have whatever knowledge I do, since otherwise I'd just be talking out my ass. Then I'll go over the most important things to remember about this kind of stuff, basically a fact-or-myth about dealing with depression. I'll finish up with some opinions about more general handling of mental health issues, because what's a blog without opinion?&lt;/p&gt;
&lt;h2&gt;Background Info&lt;/h2&gt;
&lt;p&gt;I've never told this to anyone without a medical degree, but in March 2012 I was diagnosed with mild-to-moderate depression. Though this was aggravated by a borderline case of anaemia, there's some evidence to suggest I've been dealing with depression since I was a pre-teen, most notably some hereditary predisposition and, oh yeah, &lt;em&gt;the fact that I'm still suffering from it&lt;/em&gt;. Mild-to-moderate depression is basically what we'd call "high-functioning" if I were an alcoholic: it doesn't strongly affect how I go about my day-to-day life (&lt;em&gt;usually&lt;/em&gt;), but it can still lead to significant problems down the road. What it basically boils down to is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;little or no motivation to do pretty much anything&lt;/li&gt;
&lt;li&gt;extremely low social energy at times&lt;/li&gt;
&lt;li&gt;tendency towards self-defeating thought processes&lt;/li&gt;
&lt;li&gt;a few other things I won't discuss here because my blog is public&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(I don't want to even fathom the number of people who read that section and went, "&lt;em&gt;Oh.&lt;/em&gt; That explains a lot.")&lt;/p&gt;
&lt;p&gt;Why did I never tell anyone? Well, &lt;em&gt;have you met me&lt;/em&gt;? Do I seem like the type to ask outside myself for help? Do I seem like the type to &lt;em&gt;stop and admit that something is wrong&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;I guess I'm doing my best to change that, now. (This post might not stay up forever.)&lt;/p&gt;
&lt;p&gt;"But Jacob," you might say if you skipped the rest of this section, "you talk about severe depression later on. Surely you can't be as informed about that."&lt;/p&gt;
&lt;!-- Get this section approved --&gt;

&lt;p&gt;I'd respond with this ever-so-slightly more public information. In October 2010, I started dating a wonderful woman named Monica Post. This was somewhat against my better judgement because, while I was almost certain that I was madly in love with her, I was more certain that she was severely depressed, because I'd been one of the people to help her out of a particularly low point. Later we found out that she had Bipolar Disorder, an illness that is characterized by unpredictable shifts between periods of mania (over-confidence, delusions of grandeur, hyperactivity, hallucinations, psychosis, and general emotional instability) and periods of depression (which I described above, except stronger). In a move seen by those who knew as uncharacteristically mature and brave (I don't share this belief), I stayed by her for a little over three years, through the best times and the worst.&lt;/p&gt;
&lt;p&gt;I really wish I didn't have to write in past tense. Monica died just under 4 months ago.&lt;/p&gt;
&lt;p&gt;Aside:I've never openly written on this subject without some level of control over my audience. My level of "handling it" fluctuates from day to day. On the better days, I want the subject as far away from me as possible. On the worse ones, I'm not strong enough to write about it. This blog post is partially me saying, "Fuck that. I'm stronger than this." (Before you agree with me, keep in mind that I had to write this section in private because I'm shaking like a poorly-built house on a fault line.)&lt;/p&gt;
&lt;p&gt;So uh yeah. Life experience is a bitch.&lt;/p&gt;
&lt;h2&gt;Facts&lt;/h2&gt;
&lt;p&gt;(Or as close to facts as I can get)&lt;/p&gt;
&lt;p&gt;First and foremost, something I feel should always be mentioned: depression is mostly about chemical imbalances in the brain. If there's anything that bugs knowledgeable people the most, it's when people say to just "Get over it." Seriously, fuck you. That's like telling a person in a wheelchair to just get up and walk.&lt;/p&gt;
&lt;p&gt;Now let's go over some treatment methods. The "simplest" treatment is psychotherapy, or, if you're in a lazy work of fiction, "lie down on a couch with a guy who looks like Freud and talk about your mother." The current main school of thought is &lt;a href="http://en.wikipedia.org/wiki/Cognitive_behavioral_therapy"&gt;Cognitive Behavioural Therapy&lt;/a&gt;, which focuses on harmful thought processes and destructive emotional responses. I say "main", but from what I've experienced I could also easily say "only," because I've never seen any other kind of psychotherapy suggested. In programming terms, CBT (as it's almost always abbreviated) is debugging your mind: constantly thinking, "Why did I think that? Where did that emotional outburst come from? Why am I so affected by this?" Luckily, most people are better at this than &lt;code&gt;gdb&lt;/code&gt;, since it's been shown to be fairly effective on anything short of severe depression (not to mention many other kinds of mental illness.) Hell, I'd say depression-free people can even benefit from it from time to time.&lt;/p&gt;
&lt;p&gt;But sometimes it just doesn't work. You'd be surprised at how self-defeating a person with depression can be, and sometimes it's just too deeply-rooted in their thought process to be dislodged like that. Or it just takes too much energy, and depression takes enough away on its own. For that, medication is the next best thing. &lt;/p&gt;
&lt;p&gt;Remember how I said depression is mainly chemicals? See, brains are &lt;em&gt;hard&lt;/em&gt;. There's so much that can go wrong, and, even when it looks like the same thing is wrong in two people, they may not be helped by the same thing. Again, in CS-y terms, it's like supporting an Android app on every phone: there are so many subtle differences in hardware and the implementation of software that a bugfix on one platform is nowhere near guaranteed to work on all others. Hell, it might make things &lt;em&gt;worse&lt;/em&gt;. That's why there are so many anti-depressants out there, and that's why some people spend their entire life finding the right one. (Ignore the implications of the wording. That's not what I meant.) Prozac might work for you, or it might make you more suicidal. Effexor might make you a little brighter, or it might numb your emotions. Cymbalta might make you more stable or (also likely) less. But, of course, SSRIs don't work that fast, so you won't know until 4 weeks in, and by then you'll be experiencing all the &lt;em&gt;negative&lt;/em&gt; side-effects, and you'll probably go through withdrawal if it turns out it wasn't even working. Then you just repeat the process the next time you can see your psychiatrist, because, wow, it looks like they're booked &lt;em&gt;through until December&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Sorry, that's a bit of a raw nerve. Back on topic...&lt;/p&gt;
&lt;p&gt;There are more extreme cures and fixes than medication and thought-correction, but I won't go into them here because I &lt;em&gt;won't&lt;/em&gt; be able to write about them while staying in a stable frame of mind. There are also less-extreme things that help. A huge factor, for instance, is proper physical health. I've noticed personally that it's easier to handle my symptoms when I've been keeping up a healthy routine of diet and exercise (though maintaining that through an episode isn't easy, to say the least). Obviously this isn't a cure, since the episodes still happen, but it has been shown to improve the stress reaction, which I think is the most important factor in getting through it. (&lt;a href="http://www.webmd.com/depression/guide/exercise-depression"&gt;Source&lt;/a&gt;) To put this in nerd-terms, this goes about as far towards computer performance as keeping your fan free of dust: it won't fix a crashed hard-drive, but it'll help out at times.&lt;/p&gt;
&lt;h2&gt;Opinion&lt;/h2&gt;
&lt;p&gt;Where to begin?&lt;/p&gt;
&lt;p&gt;Let's start off at the obvious: please treat people with depression (and any other mental illness) as &lt;em&gt;people&lt;/em&gt;. Like, they aren't animals in a zoo or something, nor are they that mask of tragedy that hangs along its creepy-smile companion over every middle school drama department. They have just as much depth to their emotional state as any other person, and they &lt;em&gt;really&lt;/em&gt; don't like to be labelled. I know the party-line is always about "removing stigma," but I feel like a lot of people who talk like that do very little to actually &lt;em&gt;remove&lt;/em&gt; it, and in some ways actually &lt;em&gt;further&lt;/em&gt; it.&lt;/p&gt;
&lt;p&gt;On that note, the idea of a "mental health rally?" Stop. The last thing a depressed person wants is to be reminded of it in public. Further, you're making a social event &lt;em&gt;for people with no energy for social activity&lt;/em&gt;. How does that make sense? Making people aware of resources available to them is one thing. Pointing out what's &lt;em&gt;wrong&lt;/em&gt; in a public setting is another.&lt;/p&gt;
&lt;p&gt;If you're having trouble, &lt;em&gt;see a therapist at least once&lt;/em&gt;. I'm not saying to sign up for 10 sessions, just see if it helps and go from there. If you want a resource on CBT for some self-study, I found &lt;a href="http://www.amazon.com/Mind-Over-Mood-Change-Changing/dp/0898621283"&gt;Mind over Mood&lt;/a&gt; to be pretty helpful. &lt;/p&gt;
&lt;p&gt;Lastly, don't suffer in silence. Don't wait 10 years to tell your family you think something is wrong. Maybe don't blog about it, but talking to someone, &lt;em&gt;anyone&lt;/em&gt;, will help, trust me. I'm probably pushing the blog-barrier again, but that's one of the things I miss most, which is likely why I'm writing about it now, as opposed to months ago.&lt;/p&gt;
&lt;p&gt;Happy 3rd post! I'm going to go sit in my room for a while.&lt;/p&gt;
&lt;p&gt;Later,&lt;/p&gt;
&lt;p&gt;J&lt;/p&gt;</summary><category term="personal"></category><category term="mental health"></category><category term="heavy"></category></entry><entry><title>Pelican, or, How I came to despise Ruby</title><link href="http://jabrahams.ca/pelican.html" rel="alternate"></link><updated>2014-03-02T09:15:00Z</updated><author><name>Jacob Abrahams</name></author><id>tag:jabrahams.ca,2014-03-02:pelican.html</id><summary type="html">&lt;p&gt;This blog has been in the works for...well...a while now.&lt;/p&gt;
&lt;p&gt;Maybe a year, give or take.&lt;/p&gt;
&lt;p&gt;I bought the domain name back in October in the hopes of having something to show off to interviewers. When it came down to actually making the site, I ran into some issues.&lt;/p&gt;
&lt;p&gt;Namely, I had no idea how to do that.&lt;/p&gt;
&lt;p&gt;To give myself some credit, I did know a thing or two of HTML, CSS, Javascript, jQuery, and nodejs (which is what I was initially using to serve pages, from within a heroku app). But I didn't know good design practices for web (how do i shot CSS), had a lot of out-of-date information (I blame learning HTML back at 1.0), and was too cocky to just delegate all the hard decisions to an existing framework.&lt;/p&gt;
&lt;p&gt;I chose to fix the last one.&lt;/p&gt;
&lt;p&gt;The result was a broken mess that I couldn't bear to link to on my resume out of shame (but will link to &lt;a href="http://jabrahams-1.herokuapp.com"&gt;here&lt;/a&gt; out of shamelessness). So yesterday, I sat down and made some changes, because I felt work-sick and knew the only way to remedy that was to stare at text files for hours on end. I made a &lt;a href="http://github.com/JacobLuke/jacobluke.github.io"&gt;github.io repository&lt;/a&gt; and picked a framework: Octopress. It came well-recommended by various acquaintances, it had a neat aesthetic, and the deploy process looked pretty straightforward.&lt;/p&gt;
&lt;p&gt;That mistake cost me 4 hours.&lt;/p&gt;
&lt;p&gt;Let me elaborate a little. I run Windows, partially because I have been for years, partially because I like being able to game, and mostly because of inertia. (I've run Mint and Ubuntu in the past and will probably reinstall one of them at one point.) Octopress is built in Ruby. If there's one programming language community that utterly despises Windows users, it's...well, it's Objective-C, but Ruby's a close second. Basically every step of the build process had to be finagled into working. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ruby 2.0.0 doesn't work? Ruby 1.9.3 gets you one step further before breaking.&lt;/li&gt;
&lt;li&gt;Ruby just won't build? Need the not-optional-but-also-not-pre-bundled devkit.&lt;/li&gt;
&lt;li&gt;Gems won't install? Oh, by default they're sent as &lt;em&gt;binaries&lt;/em&gt;, which are obviously not going to work on a completely different operating system.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before I even found the answer to that third one, I was already looking for alternatives. Github pages (like this one) ship with Jekyll by default, but that was apparently just a less-well-loved Octopress. Plus it was still in Ruby. I considered not using a publishing platform, just injecting Bootstrap into the page to make it a little prettier, but decided that I'd rather do something a little more. I even considered writing my own publishing platform, but realized that was way beyond my effort-threshold for this blog. And then I found Pelican.&lt;/p&gt;
&lt;p&gt;Pelican is a blogging framework written in Python. This had a ton of advantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I know that Python at least runs on Windows, since it's what I do most of my non-work day-to-day coding in&lt;/li&gt;
&lt;li&gt;Python projects typically have at least half-decent support for people chained to Microsoft. I actually find it's often better than their Mac support.&lt;/li&gt;
&lt;li&gt;I know enough of the language to debug things when stuff goes wrong, or potentially extend/contribute the project in some way.&lt;/li&gt;
&lt;li&gt;Usually the python community is nice people. Usually.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So I uninstalled Ruby (both versions that had made their way to my machine), deleted all the random repositories I'd checked out trying to make it work, reset all the settings that the installers had broken, and checked out Pelican. Or tried to, realized that my &lt;code&gt;PATH&lt;/code&gt; environment variable was missing most of the subdirectories of my Python installation, fixed that, and got everything working-enough to actually publish an article. And, after some early-morning wrangling with my Host configurations, this blog was born.&lt;/p&gt;
&lt;p&gt;To sum up:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Don't use Windows&lt;/li&gt;
&lt;li&gt;If you absolutely must use Windows, don't use Ruby. Use Python.&lt;/li&gt;
&lt;li&gt;When in doubt, add more stuff to &lt;code&gt;PATH&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now excuse me while I hunt down a configuration setting.&lt;/p&gt;
&lt;p&gt;J&lt;/p&gt;</summary><category term="tech"></category><category term="nerdish stuff"></category><category term="programming"></category><category term="metablogging"></category></entry><entry><title>A blog</title><link href="http://jabrahams.ca/a-blog.html" rel="alternate"></link><updated>2014-03-02T01:02:00Z</updated><author><name>Jacob Abrahams</name></author><id>tag:jabrahams.ca,2014-03-02:a-blog.html</id><summary type="html">&lt;p&gt;Well now.&lt;/p&gt;
&lt;p&gt;I'd be lying if I said I haven't tried my hand at this before, but I'm giving it another shot. Plus, it's tied to my professional identity this time, which is kind of like holding my own career hostage.&lt;/p&gt;
&lt;p&gt;Hi. I'm Jacob. Nice to meet you.&lt;/p&gt;
&lt;p&gt;I do a bunch of things. I go to school at the University of Waterloo (in the frozen North). I intern at companies from time to time. (My &lt;a href="http://linkedin.com/in/jlabrahams"&gt;LinkedIn profile&lt;/a&gt; is a good source of truth on what I'm up to in that regard.) I write software from time to time, mostly simple I-wonder-how-fast-I-can-whip-this-up-in-Python things. I read books and listen to music, and when asked can never describe my taste in either, but usually have a point of view on whatever. I play some music too, mostly classical piano. I play video games, though it's kind of like my book-tastes: I never really know which ones. I bike places when my bike isn't broken, which is rare. I like good beer (lager in warm weather, amber ales in the winter), good wine (mostly Riesling and Chardonnay), and good company.&lt;/p&gt;
&lt;p&gt;I don't know what I want this blog to be yet. I have some programming-related rants that have been kicking around in my head for a while, and some short-story ideas I'd want to flesh out a little more. I try to stay away from politics, but who knows? If you're still reading at this point, chances are better-than-even that you know I haven't exactly had the best run of luck the past couple of months, and I'm on the fence about just how much of that I want to get off my chest in such a public and tied-to-me way. Or I might just talk about things I don't know anything about. Yeah, probably that one.&lt;/p&gt;
&lt;p&gt;So there it is. Basically who I am in two paragraphs. If my writing style hasn't driven you away yet, it probably won't. So sit and stay awhile.&lt;/p&gt;
&lt;p&gt;I'm sure it'll be worth it.&lt;/p&gt;
&lt;p&gt;J&lt;/p&gt;</summary><category term="metablogging"></category></entry><entry><title>Eulogy</title><link href="http://jabrahams.ca/eulogy.html" rel="alternate"></link><updated>2013-11-18T00:00:00Z</updated><author><name>Jacob Abrahams</name></author><id>tag:jabrahams.ca,2013-11-18:eulogy.html</id><summary type="html">&lt;p&gt;(&lt;strong&gt;Note: I've re-dated this article so it doesn't turn up at the top of the page anymore. As you might tell from context, it was written a year after the publish date. Sorry for the confusion.&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;I never really did say a proper eulogy for Monica. I mean, sure, I said something at the service, and I guess it sufficed. I couldn't &lt;em&gt;not&lt;/em&gt;, not after what I'd shared with her, not with the raw pain I felt that day. My memory's a little broken (like everything else), but I remember bits and pieces of what I said, or set out to say, with my already-lacking public speaking skill set ever lower by circumstance. Something about having hoped to gather everyone in a church for a much happier reason. Something about how she'd lit up everyone's life. Something about her insisting on making gingerbread at 2AM one night when were supposed to be packing to leave our apartment for two weeks. Something about making too much gingerbread dough and most of it going bad.&lt;/p&gt;
&lt;p&gt;But I can do better. I'm a writer, after all, not a speaker. It's only a year late.&lt;/p&gt;
&lt;p&gt;Let me first share a story that was shared with me a few years back. This is way back, around the time Monica was 3 or 4 if I recall correctly. Her family was on vacation somewhere seaside, and her mom was watching over her somewhere on a beach. And Monica was intent on swimming. "Monica swim," she insisted. No, she was told, you're too little. "Monica swim," she replied, adamant. No, she was instructed, you'll fall. "Monica swim," she said one last time, and walked off to the water. Of course, she immediately fell in, but her mom was right there to pluck her out of the water to safety. As she spat out the saltwater, she had one last thing to say: "Monica no swim."&lt;/p&gt;
&lt;p&gt;I think this shows the first thing a lot of people notice about her. Even when she was too young to understand grammar or tides, she still had this &lt;em&gt;determination&lt;/em&gt; to do what she thought needed to be done. She was one of the most determined people I knew, rarely letting anything get in the way of her goals. Sometimes, sure, she'd get in over her head, but she always managed to get plucked out out of the water.&lt;/p&gt;
&lt;p&gt;When I think of the Monica I knew, the woman she was with me, the memories that stick with me are always the same. Sometimes, when she was feeling sad or anxious, she'd lay her head in my lap and ask me to stroke her hair until she fell asleep. I can still feel the back of her head across my thighs, her soft hair around my fingertips. In my dreams, I can still see the look in her eyes change from distressed to calm to restful, still feel the love in them behind it all as she gazed up at me. I'd usually start to fall asleep before she would, so we usually wouldn't stay like that. She asked me once if I was bored of it, but the problem was the complete opposite: looking down at her, as I held her safe, I couldn't help but be at peace with the world, because no matter what, there was at least one thing right with it.&lt;/p&gt;
&lt;p&gt;I think the phrase "better half" is overused when it comes to significant others, but I firmly believe it here: she was the missing piece of my two-piece puzzle, and she was definitely the better piece. I can't even begin to describe what she was to me, what she was to others, without feeling like I'm repeating myself. Everything she was part of felt better for it. She brought a vibrancy and energy I can't even comprehend to everything she did. She cared genuinely, and rarely if ever had a bad thing to say about anyone. She was always on the move, always curious about the next thing, always dreaming up new ideas and scheming new plans, and I loved her so much for it. I could share a thousand more stories, each one more and more &lt;em&gt;her&lt;/em&gt;, because there was always a new thing to be done, a new journey to be started, and I'm just so grateful for being taken along on every single one.&lt;/p&gt;
&lt;p&gt;That she was so friendly and caring and loving was something special in itself, but that she was all that despite the hand life had dealt her, despite her circumstance and struggles, was absolutely unheard of. Not only did the world lose a bright light the day she died, it lost one that was all the brighter for the darkness that surrounded it, and I don't think it can ever truly recover that. I think a little bit of everyone that knew her died when they heard. For some of us, it was more than a little. The world is a painful place indeed when the images and thoughts you once looked to for strength, for comfort, for support, now only offer a profound agony cutting through the core of your being. I never knew I could love someone so much that it would hurt this bad when I lost them, but that's just the kind of person Monica was, and that's just the effect she had on people.&lt;/p&gt;
&lt;p&gt;This will never be enough. There will never be enough words to express what she was to me, what she was to our family, what she was to her friends and mine. I could never run through all the stories we lived during our time together, the eternities we shared in our few, far-too-short years together. But I hope this is enough to remind those who knew her of their own Monica stories, of their own memories of times with her. And I hope it's enough for those who didn't know her to understand what the rest of us are missing, why we hurt so bad even a year later. And I hope it'd be enough for her.&lt;/p&gt;
&lt;p&gt;-Jacob&lt;/p&gt;</summary><category term="Monica"></category></entry></feed>